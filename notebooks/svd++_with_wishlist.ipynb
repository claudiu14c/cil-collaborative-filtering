{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d799b9af-d742-4cac-a937-06102e652812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable, Dict, List, Set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# torch imports are not needed for this NumPy SVD++\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import os\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063fd35-caac-4ced-b13e-b49cfb58d9a2",
   "metadata": {},
   "source": [
    "Make sure that results are reproducible by using a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e73627bd-1106-4276-a498-32b44f1b5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5b6d6-37ed-40d1-b651-962c611a22c3",
   "metadata": {},
   "source": [
    "## Helper functions & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93bc867-b2d9-4cf7-9bb8-ecb13c663eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/cluster/courses/cil/collaborative_filtering/data\" # Adjust to your path\n",
    "#DATA_DIR = \"\"\n",
    "\n",
    "# Hyperparameters for SVD++ with Wishlist Integration\n",
    "SVD_N_FACTORS = 50\n",
    "SVD_LR = 0.007       # Learning rate, may need tuning\n",
    "SVD_REG = 0.04         # Regularization, may need tuning\n",
    "SVD_N_EPOCHS_VALID = 20 # Epochs for training with a validation set\n",
    "SVD_VALID_PATIENCE = 3  # Early stopping patience for validation\n",
    "SVD_N_EPOCHS_FULL = 45  # Epochs for final training on all data - based on validation set, this was the best number of epochs\n",
    "SVD_FULL_PATIENCE = 5   # Early stopping for full training (based ONLY on train RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data_df() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reads in data and splits it into training and validation sets.\"\"\"\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "    return train_df, valid_df\n",
    "\n",
    "def read_full_training_data() -> pd.DataFrame:\n",
    "    \"\"\"Reads the entire training dataset.\"\"\"\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def evaluate_model_predictions(true_ratings: np.ndarray, pred_ratings: np.ndarray) -> float:\n",
    "    \"\"\"Calculates RMSE after clipping predictions.\"\"\"\n",
    "    preds_clipped = np.clip(pred_ratings, 1.0, 5.0)\n",
    "    return root_mean_squared_error(true_ratings, preds_clipped)\n",
    "\n",
    "def evaluate_with_model(model_dict: Dict, eval_df: pd.DataFrame, \n",
    "                          pred_function: Callable[[Dict, np.ndarray, np.ndarray], np.ndarray]) -> float:\n",
    "    \"\"\"Helper to evaluate a model dictionary on a dataframe.\"\"\"\n",
    "    if eval_df is None or eval_df.empty:\n",
    "        return np.nan\n",
    "    preds = pred_function(model_dict, eval_df[\"sid\"].values, eval_df[\"pid\"].values)\n",
    "    return evaluate_model_predictions(eval_df[\"rating\"].values, preds)\n",
    "\n",
    "def make_submission(pred_fn_callable: Callable[[np.ndarray, np.ndarray], np.ndarray], filename: os.PathLike):\n",
    "    \"\"\"Makes a submission CSV file.\"\"\"\n",
    "    df_sub = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "    sid_pid_split = df_sub[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    sids_sub_vals = sid_pid_split[0].astype(int).values\n",
    "    pids_sub_vals = sid_pid_split[1].astype(int).values\n",
    "    predictions = pred_fn_callable(sids_sub_vals, pids_sub_vals)\n",
    "    df_sub[\"rating\"] = np.clip(predictions, 1.0, 5.0)\n",
    "    df_sub.to_csv(filename, index=False)\n",
    "    print(f\"Submission file created: {filename}\")\n",
    "\n",
    "def plot_training_curves(n_total_epochs_run: int, train_rmse_hist: List[float], val_rmse_hist: List[float], title_prefix: str = \"\"):\n",
    "    epochs_ran = len(train_rmse_hist)\n",
    "    if epochs_ran == 0: \n",
    "        print(f\"{title_prefix}: No training history to plot.\")\n",
    "        return\n",
    "    epochs_range = range(1, epochs_ran + 1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_range, train_rmse_hist, \"bo-\", label=\"Training RMSE\")\n",
    "    valid_val_epochs = [e for e, r in zip(epochs_range, val_rmse_hist[:epochs_ran]) if not np.isnan(r)]\n",
    "    valid_val_rmse_points = [r for r in val_rmse_hist[:epochs_ran] if not np.isnan(r)]\n",
    "    if valid_val_epochs:\n",
    "        plt.plot(valid_val_epochs, valid_val_rmse_points, \"ro-\", label=\"Validation RMSE\")\n",
    "    plt.title(f\"{title_prefix} Training & Validation RMSE (Up to {epochs_ran} Epochs)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "svdpp-wishlist-markdown",
   "metadata": {},
   "source": [
    "## SVD++ with Integrated Wishlist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svdpp-wishlist-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svdpp_with_wishlist_integrated(train_df: pd.DataFrame, tbr_df: pd.DataFrame, \n",
    "                                         num_factors: int, lr: float, reg: float, n_epochs: int,\n",
    "                                         valid_df: pd.DataFrame = None, \n",
    "                                         early_stopping_patience: int = 5,\n",
    "                                         evaluate_every_n_epochs: int = 1) -> Tuple[Dict, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    SVD++ with wishlist data integrated into the implicit feedback component.\n",
    "    Based on the structure of your \"train_svdpp_fast\".\n",
    "    \"\"\"\n",
    "    # 1) Remap IDs using both ratings and TBR data\n",
    "    sids_ratings = train_df[\"sid\"].unique(); pids_ratings = train_df[\"pid\"].unique()\n",
    "    sids_tbr = tbr_df[\"sid\"].unique(); pids_tbr = tbr_df[\"pid\"].unique()\n",
    "    sids_all = np.union1d(sids_ratings, sids_tbr)\n",
    "    pids_all = np.union1d(pids_ratings, pids_tbr)\n",
    "    user2ind = {sid: i for i, sid in enumerate(sids_all)}\n",
    "    item2ind = {pid: i for i, pid in enumerate(pids_all)}\n",
    "    n_users, n_items = len(sids_all), len(pids_all)\n",
    "    print(f\"SVD++ (Wishlist Integrated): {n_users} users, {n_items} items\")\n",
    "\n",
    "    # 2) Build index arrays for ratings (only from train_df)\n",
    "    train_df_mappable = train_df[train_df[\"sid\"].isin(user2ind) & train_df[\"pid\"].isin(item2ind)]\n",
    "    user_arr   = train_df_mappable[\"sid\"].map(user2ind).to_numpy(dtype=np.int32)\n",
    "    item_arr   = train_df_mappable[\"pid\"].map(item2ind).to_numpy(dtype=np.int32)\n",
    "    rating_arr = train_df_mappable[\"rating\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    # 3) Global mean from ratings\n",
    "    mu = np.float32(rating_arr.mean()) if len(rating_arr) > 0 else np.float32(3.5)\n",
    "\n",
    "    # 4) Init parameters\n",
    "    b_u = np.zeros(n_users, np.float32); b_i = np.zeros(n_items, np.float32)\n",
    "    p   = np.random.normal(0, 0.01, (n_users, num_factors)).astype(np.float32)\n",
    "    q   = np.random.normal(0, 0.01, (n_items, num_factors)).astype(np.float32)\n",
    "    y   = np.random.normal(0, 0.01, (n_items, num_factors)).astype(np.float32)\n",
    "\n",
    "    # 5) Build combined implicit lists (N'(u) = Rated(u) U Wishlisted(u))\n",
    "    print(\"Building combined implicit feedback lists (ratings + wishlist)...\")\n",
    "    implicit_combined: Dict[int, Set[int]] = {u_idx: set() for u_idx in range(n_users)}\n",
    "    for sid_orig, pid_orig in zip(train_df[\"sid\"], train_df[\"pid\"]):\n",
    "        if sid_orig in user2ind and pid_orig in item2ind:\n",
    "            implicit_combined[user2ind[sid_orig]].add(item2ind[pid_orig])\n",
    "    for sid_orig, pid_orig in zip(tbr_df[\"sid\"], tbr_df[\"pid\"]):\n",
    "        if sid_orig in user2ind and pid_orig in item2ind:\n",
    "            implicit_combined[user2ind[sid_orig]].add(item2ind[pid_orig])\n",
    "    \n",
    "    Nu_list_combined: List[np.ndarray] = [np.array(list(implicit_combined[u_idx]), dtype=np.int32) for u_idx in range(n_users)]\n",
    "    Nu_count_combined = np.array([len(a) for a in Nu_list_combined], dtype=np.int32)\n",
    "    sqrt_Nu_combined_inv = np.where(Nu_count_combined > 0, 1.0 / np.sqrt(Nu_count_combined, dtype=np.float32), np.float32(0.0))\n",
    "    sqrt_Nu_for_pred = np.where(Nu_count_combined > 0, np.sqrt(Nu_count_combined, dtype=np.float32), np.float32(1.0))\n",
    "\n",
    "    # History and Best Model Tracking\n",
    "    train_rmse_history = []\n",
    "    val_rmse_history = []\n",
    "    best_metric = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    best_model_params = {}\n",
    "\n",
    "    n_ratings = len(user_arr)\n",
    "    if n_ratings == 0:\n",
    "        print(\"SVD++ Wishlist: No ratings for training.\")\n",
    "        best_model_params = {\"mu\":mu, \"b_u\":b_u, \"b_i\":b_i, \"p\":p, \"q\":q, \"y\":y, \n",
    "                       \"user2ind\":user2ind, \"item2ind\":item2ind, \n",
    "                       \"Nu_list_combined\": Nu_list_combined, \"sqrt_Nu_for_pred\": sqrt_Nu_for_pred, \n",
    "                       \"num_factors\":num_factors}\n",
    "        return best_model_params, [], []\n",
    "\n",
    "    print(f\"SVD++ Wishlist: Starting SGD ({n_epochs} epochs on {n_ratings} ratings)...\")\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_epoch_squared_errors = []\n",
    "        \n",
    "        # Recompute y_sum based on Nu_list_combined (includes wishlist)\n",
    "        y_sum = np.zeros((n_users, num_factors), np.float32) # y_sum is sum(y_j)/sqrt(N'_u)\n",
    "        for u_idx in range(n_users):\n",
    "            if Nu_count_combined[u_idx] > 0:\n",
    "                y_sum[u_idx] = y[Nu_list_combined[u_idx]].sum(axis=0) / sqrt_Nu_for_pred[u_idx]\n",
    "\n",
    "        perm = np.random.permutation(n_ratings)\n",
    "        for i_perm_idx in range(n_ratings):\n",
    "            idx = perm[i_perm_idx]\n",
    "            u = user_arr[idx]; i = item_arr[idx]; r = rating_arr[idx]\n",
    "            \n",
    "            imp = y_sum[u] # This now uses the combined implicit feedback\n",
    "            pred = mu + b_u[u] + b_i[i] + q[i].dot(p[u] + imp)\n",
    "            err  = r - pred\n",
    "            current_epoch_squared_errors.append(err**2)\n",
    "\n",
    "            b_u[u] += lr * (err - reg * b_u[u])\n",
    "            b_i[i] += lr * (err - reg * b_i[i])\n",
    "            p_old  = p[u].copy()\n",
    "            p[u]  += lr * (err * q[i] - reg * p[u])\n",
    "            q[i]  += lr * (err * (p_old + imp) - reg * q[i])\n",
    "\n",
    "            # Update y_j for items in Nu_list_combined[u]\n",
    "            if Nu_count_combined[u] > 0:\n",
    "                # sqrt_Nu_combined_inv[u] is 1/sqrt(N'_u)\n",
    "                coeff_grad_y = err * q[i] * sqrt_Nu_combined_inv[u] \n",
    "                idxs  = Nu_list_combined[u]\n",
    "                y[idxs] += lr * (coeff_grad_y - reg * y[idxs])\n",
    "                \n",
    "                # Update y_sum[u] incrementally\n",
    "                y_sum[u] += lr * (err * q[i] * sqrt_Nu_combined_inv[u] - reg * y_sum[u])\n",
    "\n",
    "        epoch_train_rmse = np.sqrt(np.mean(current_epoch_squared_errors))\n",
    "        train_rmse_history.append(epoch_train_rmse)\n",
    "        current_val_rmse = np.nan\n",
    "        stopping_metric = epoch_train_rmse\n",
    "\n",
    "        if valid_df is not None and not valid_df.empty and \\\n",
    "           ((epoch + 1) % evaluate_every_n_epochs == 0 or epoch == n_epochs - 1):\n",
    "            temp_model_params = {\n",
    "                \"mu\":mu, \"b_u\":b_u, \"b_i\":b_i, \"p\":p, \"q\":q, \"y\":y,\n",
    "                \"user2ind\":user2ind, \"item2ind\":item2ind,\n",
    "                \"Nu_list_combined\": Nu_list_combined, # Pass combined list\n",
    "                \"sqrt_Nu_for_pred\": sqrt_Nu_for_pred, # Pass corresponding sqrt\n",
    "                \"num_factors\":num_factors\n",
    "            }\n",
    "            current_val_rmse = evaluate_with_model(temp_model_params, valid_df, svdpp_pred_wishlist_integrated)\n",
    "            val_rmse_history.append(current_val_rmse)\n",
    "            stopping_metric = current_val_rmse\n",
    "            print(f\"  SVD++ Wishlist: Epoch {epoch+1}/{n_epochs}. Train RMSE: {epoch_train_rmse:.4f}, Valid RMSE: {current_val_rmse:.4f}. Took: {time.time() - epoch_start_time:.2f}s.\")\n",
    "        else:\n",
    "            val_rmse_history.append(np.nan)\n",
    "            print(f\"  SVD++ Wishlist: Epoch {epoch+1}/{n_epochs}. Train RMSE: {epoch_train_rmse:.4f}. Took: {time.time() - epoch_start_time:.2f}s. (Val skipped)\")\n",
    "\n",
    "        if stopping_metric < best_metric:\n",
    "            best_metric = stopping_metric\n",
    "            epochs_no_improve = 0\n",
    "            best_model_params = {\n",
    "                \"mu\": mu, \"b_u\": b_u.copy(), \"b_i\": b_i.copy(),\n",
    "                \"p\": p.copy(), \"q\": q.copy(), \"y\": y.copy(),\n",
    "                \"user2ind\": user2ind, \"item2ind\": item2ind,\n",
    "                \"Nu_list_combined\": [arr.copy() for arr in Nu_list_combined], \n",
    "                \"sqrt_Nu_for_pred\": sqrt_Nu_for_pred.copy(),\n",
    "                \"num_factors\": num_factors\n",
    "            }\n",
    "            print(f\"    New best metric ({\"Valid\" if not np.isnan(current_val_rmse) else \"Train\"} RMSE): {best_metric:.4f}. Model params saved.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if early_stopping_patience > 0 and epochs_no_improve >= early_stopping_patience:\n",
    "            print(f\"    SVD++ Wishlist: Early stopping at epoch {epoch+1}.\")\n",
    "            break\n",
    "            \n",
    "    if not best_model_params: # If loop didnt run or improve\n",
    "        print(\"SVD++ Wishlist: No best model found, returning last state.\")\n",
    "        best_model_params = {\"mu\":mu, \"b_u\":b_u, \"b_i\":b_i, \"p\":p, \"q\":q, \"y\":y, \n",
    "                       \"user2ind\":user2ind, \"item2ind\":item2ind, \n",
    "                       \"Nu_list_combined\": Nu_list_combined, \"sqrt_Nu_for_pred\": sqrt_Nu_for_pred, \n",
    "                       \"num_factors\":num_factors}\n",
    "    return best_model_params, train_rmse_history, val_rmse_history\n",
    "\n",
    "def svdpp_pred_wishlist_integrated(model: Dict, sids_arr: np.ndarray, pids_arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Prediction function for SVD++ model trained with integrated wishlist data.\"\"\"\n",
    "    mu = model[\"mu\"]\n",
    "    user2ind = model[\"user2ind\"]\n",
    "    item2ind = model[\"item2ind\"]\n",
    "    b_u = model[\"b_u\"]\n",
    "    b_i = model[\"b_i\"]\n",
    "    p = model[\"p\"]\n",
    "    q = model[\"q\"]\n",
    "    y = model[\"y\"]\n",
    "    num_factors = model[\"num_factors\"]\n",
    "    Nu_list_combined = model[\"Nu_list_combined\"] # Use the combined list\n",
    "    sqrt_Nu_for_pred = model[\"sqrt_Nu_for_pred\"] # Use corresponding sqrt factor\n",
    "    \n",
    "    n_preds = len(sids_arr)\n",
    "    preds = np.full(n_preds, mu, dtype=np.float32)\n",
    "\n",
    "    for k_idx in range(n_preds):\n",
    "        sid = sids_arr[k_idx]\n",
    "        pid = pids_arr[k_idx]\n",
    "        \n",
    "        current_pred = mu \n",
    "        if (sid in user2ind) and (pid in item2ind):\n",
    "            u = user2ind[sid]\n",
    "            i = item2ind[pid]\n",
    "            \n",
    "            user_implicit_items = Nu_list_combined[u]\n",
    "            norm_factor = sqrt_Nu_for_pred[u]\n",
    "            \n",
    "            imp_sum = np.zeros(num_factors, dtype=np.float32)\n",
    "            if user_implicit_items.size > 0 and norm_factor > 1e-9: \n",
    "                imp_sum = np.sum(y[user_implicit_items], axis=0) / norm_factor\n",
    "            \n",
    "            current_pred = mu + b_u[u] + b_i[i] + np.dot(q[i], p[u] + imp_sum)\n",
    "        preds[k_idx] = current_pred\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-routine-markdown",
   "metadata": {},
   "source": [
    "### Main routine for training and evaluation (using SVD++ with Wishlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-routine-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary data\n",
    "train_df_split, valid_df_split = read_data_df() # For validation run\n",
    "tbr_df_global = pd.read_csv(os.path.join(DATA_DIR, \"train_tbr.csv\"))\n",
    "\n",
    "print(\"--- Training SVD++ with Integrated Wishlist (on validation split) ---\")\n",
    "svdpp_wishlist_model_val, train_hist_val, val_hist_val = train_svdpp_with_wishlist_integrated(\n",
    "    train_df_split, \n",
    "    tbr_df_global, \n",
    "    num_factors=SVD_N_FACTORS, \n",
    "    lr=SVD_LR, \n",
    "    reg=SVD_REG, \n",
    "    n_epochs=SVD_N_EPOCHS_VALID,\n",
    "    valid_df=valid_df_split,\n",
    "    early_stopping_patience=SVD_VALID_PATIENCE,\n",
    "    evaluate_every_n_epochs=1\n",
    ")\n",
    "\n",
    "plot_training_curves(SVD_N_EPOCHS_VALID, train_hist_val, val_hist_val, \"SVD++ Integrated Wishlist (Validation Run)\")\n",
    "final_val_rmse = val_hist_val[-1] if val_hist_val and not np.isnan(val_hist_val[-1]) else train_hist_val[-1]\n",
    "print(f\"Final RMSE on validation set (SVD++ with Wishlist): {final_val_rmse:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Training SVD++ with Integrated Wishlist (on FULL data for submission) ---\")\n",
    "full_train_data = read_full_training_data()\n",
    "svdpp_wishlist_model_full, train_hist_full, _ = train_svdpp_with_wishlist_integrated(\n",
    "    full_train_data, \n",
    "    tbr_df_global, \n",
    "    num_factors=SVD_N_FACTORS, \n",
    "    lr=SVD_LR, \n",
    "    reg=SVD_REG, \n",
    "    n_epochs=SVD_N_EPOCHS_FULL, # Use full epochs here\n",
    "    valid_df=None, # No separate validation set for full training\n",
    "    early_stopping_patience=SVD_FULL_PATIENCE, # Based on training RMSE\n",
    "    evaluate_every_n_epochs=1 # Will only print train RMSE\n",
    ")\n",
    "plot_training_curves(SVD_N_EPOCHS_FULL, train_hist_full, [], \"SVD++ Integrated Wishlist (Full Training)\")\n",
    "print(f\"Lowest Training RMSE on full data: {min(train_hist_full):.4f}\")\n",
    "\n",
    "# Make submission\n",
    "submission_pred_fn = lambda sids_arr, pids_arr: svdpp_pred_wishlist_integrated(svdpp_wishlist_model_full, sids_arr, pids_arr)\n",
    "submission_filename = f\"svdpp_integrated_wishlist_f{SVD_N_FACTORS}_lr{SVD_LR}_reg{SVD_REG}_ep{len(train_hist_full) if train_hist_full else SVD_N_EPOCHS_FULL}.csv\"\n",
    "make_submission(submission_pred_fn, submission_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
