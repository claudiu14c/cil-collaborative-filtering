{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0003cb4-18fb-4ad8-b5d7-24a1626e066c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Collaborative filtering project\n",
    "\n",
    "In this project, the task is to create a paper recommendation system. The system consists of 10,000 scientists and 1,000 papers. Scientists give ratings between 1â€“5 to the papers that they read. Since not all scientists have read every paper, we only have a limited amount of observations of these ratings. Additionally, each scientist has a wishlist of papers that they would like to read in the future. Your task is to fill in the missing observations using the provided rating and wishlist data, such that we can recommend papers to scientists that we expect them to rate highly.\n",
    "\n",
    "More specifically, there are three data sources:\n",
    " - `train_tbr.csv` containing wishlist data.\n",
    " - `train_ratings.csv` containing observed rating data.\n",
    " - `sample_submission.csv` containing (scientist, paper) pairs that have to be rated for the evaluation of your method.\n",
    "\n",
    "The data is available at `/cluster/courses/cil/collaborative_filtering/data` and an environment has been prepared for you at `/cluster/courses/cil/envs/collaborative_filtering`. You can activate the environment in your shell by running:\n",
    "```bash\n",
    "conda activate /cluster/courses/cil/envs/collaborative_filtering\n",
    "```\n",
    "If you wish to use notebooks on the cluster, you need to set the Environment path to `/cluster/courses/cil/envs/collaborative_filtering/bin` and load the `cuda/12.6` module.\n",
    "\n",
    "**Evaluation**: Your models are evaluated using the root mean-squared error (RMSE) metric. Your grade is determined by a linear interpolation between the easy (grade 4) and hard (grade 6) baselines.\n",
    "\n",
    "**Rules**: You are only allowed to use the data provided in `train_tbr.csv` and `train_ratings.csv` to make your predictions of `sample_submission.csv`. You are not allowed to use external data sources. But, you are allowed to use pre-trained models, as long as they are available publicly. Furthermore, no external API calls are allowed, except for downloading the weights of pre-trained models.\n",
    "\n",
    "**We will verify your code for plagiarism and using solutions from previous years.**\n",
    "\n",
    "[Link to Kaggle competition](https://www.kaggle.com/competitions/ethz-cil-collaborative-filtering-2025)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08604754-96c4-40dc-8d77-16be2dd6a36e",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d799b9af-d742-4cac-a937-06102e652812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9afc16-e53b-4ed9-bc2d-88113cb7d844",
   "metadata": {},
   "source": [
    "Make sure that results are reproducible by using a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73627bd-1106-4276-a498-32b44f1b5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33120a96-bc2c-48f3-8d46-72b932faa021",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93bc867-b2d9-4cf7-9bb8-ecb13c663eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/cluster/courses/cil/collaborative_filtering/data\"\n",
    "\n",
    "\n",
    "def read_data_df() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reads in data and splits it into training and validation sets with a 75/25 split.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "\n",
    "    # Split sid_pid into sid and pid columns\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "    \n",
    "    # Split into train and validation dataset\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.01)\n",
    "    return train_df, valid_df\n",
    "\n",
    "\n",
    "def evaluate(valid_df: pd.DataFrame, pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        valid_df: Validation data, returned from read_data_df for example.\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs their rating predictions.\n",
    "\n",
    "    Outputs: Validation RMSE\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = pred_fn(valid_df[\"sid\"].values, valid_df[\"pid\"].values)\n",
    "    return root_mean_squared_error(valid_df[\"rating\"].values, preds)\n",
    "\n",
    "\n",
    "def make_submission(pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray], filename: os.PathLike):\n",
    "    \"\"\"Makes a submission CSV file that can be submitted to kaggle.\n",
    "\n",
    "    Inputs:\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs a score.\n",
    "        filename: File to save the submission to.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "    # Get sids and pids\n",
    "    sid_pid = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    sids = sid_pid[0]\n",
    "    pids = sid_pid[1]\n",
    "    sids = sids.astype(int).values\n",
    "    pids = pids.astype(int).values\n",
    "    \n",
    "    df[\"rating\"] = pred_fn(sids, pids)\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f0694-24d9-4b8f-ad15-d89853bfbb9a",
   "metadata": {},
   "source": [
    "Try to use a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f961a4-09d3-4c1b-951d-68d304688e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0dbb75a-1a11-4ac4-aa39-a3899a5db0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "class NeuralCollaborativeFilteringModel(nn.Module):\n",
    "    def __init__(self, num_scientists: int, num_papers: int, dim: int, hidden_dims=(32,16)):\n",
    "        super().__init__()\n",
    "\n",
    "        # Assign to each scientist and paper an embedding\n",
    "        self.scientist_vec_gmd = nn.Embedding(num_scientists, dim)\n",
    "        self.paper_vec_gmd = nn.Embedding(num_papers, dim)\n",
    "\n",
    "        self.scientist_emb_mlp = nn.Embedding(num_scientists, dim)\n",
    "        self.paper_emb_mlp = nn.Embedding(num_papers, dim)\n",
    "\n",
    "        # MLP layers\n",
    "        mlp1_layers = []\n",
    "        input_dim = dim * 2  # because we concatenate two embeddings\n",
    "\n",
    "        for hdim in hidden_dims:\n",
    "            mlp1_layers.append(nn.Linear(input_dim, hdim))\n",
    "            mlp1_layers.append(nn.ReLU())\n",
    "            mlp1_layers.append(nn.Dropout(0.2))\n",
    "            input_dim = hdim\n",
    "\n",
    "        self.mlp1 = nn.Sequential(*mlp1_layers)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Final prediction layer\n",
    "        output_layers = []\n",
    "        output_layers.append(nn.Linear(dim + hidden_dims[-1], 1))\n",
    "        output_layers.append(nn.ReLU())\n",
    "        self.output_layer = nn.Sequential(*output_layers)\n",
    "        \n",
    "    def forward(self, sid: torch.Tensor, pid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            sid: [B,], int\n",
    "            pid: [B,], int\n",
    "        \n",
    "        Outputs: [B,], float\n",
    "        \"\"\"\n",
    "        \n",
    "        # Fetch gmd embeddings\n",
    "        scientist_vec_gmd = self.scientist_vec_gmd(sid)  # [B, dim]\n",
    "        paper_vec_gmd = self.paper_vec_gmd(pid)          # [B, dim]\n",
    "\n",
    "        gmf = scientist_vec_gmd * paper_vec_gmd\n",
    "\n",
    "        # Fetch mlp embeddings\n",
    "        scientist_vec = self.scientist_emb_mlp(sid)  # [B, dim]\n",
    "        paper_vec = self.paper_emb_mlp(pid)          # [B, dim]\n",
    "\n",
    "\n",
    "        # # Concatenate embeddings\n",
    "        x = torch.cat([scientist_vec, paper_vec], dim=-1)  # [B, 2*dim]\n",
    "\n",
    "        # # Feed through MLP\n",
    "        x = self.mlp1(x)  # [B, hdims[-1]]\n",
    "\n",
    "        x = torch.cat([x, gmf], dim=-1) # [B, dim+hdims[-1]]\n",
    "\n",
    "        # Final output\n",
    "        x = self.output_layer(x)  # [B, 1]\n",
    "\n",
    "        x = x.squeeze(-1)  # [B]\n",
    "\n",
    "        return x\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9612520-bc2b-438f-bc2d-6fe7785af86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model (10k scientists, 1k papers, 32-dimensional embeddings) and optimizer\n",
    "model = NeuralCollaborativeFilteringModel(10_000, 1_000, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2affbcf-993c-4504-861c-5e0e7a0c81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(df: pd.DataFrame) -> torch.utils.data.Dataset:\n",
    "    \"\"\"Conversion from pandas data frame to torch dataset.\"\"\"\n",
    "    \n",
    "    sids = torch.from_numpy(df[\"sid\"].to_numpy())\n",
    "    pids = torch.from_numpy(df[\"pid\"].to_numpy())\n",
    "    ratings = torch.from_numpy(df[\"rating\"].to_numpy()).float()\n",
    "    return torch.utils.data.TensorDataset(sids, pids, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eef4e19-bad2-4774-a247-fbe2a3ffa2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = read_data_df()\n",
    "train_dataset = get_dataset(train_df)\n",
    "valid_dataset = get_dataset(valid_df)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab0b33-af41-492b-b270-23dee71440ba",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15519a39-b77e-4a31-b32c-7c685bb51291",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "def train_model(model, train_loader):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"Epoch\", epoch)\n",
    "        # Train model for an epoch\n",
    "        total_loss = 0.0\n",
    "        total_data = 0\n",
    "        model.train()\n",
    "        for sid, pid, ratings in train_loader:\n",
    "            # Move data to GPU\n",
    "            sid = sid.to(device)\n",
    "            pid = pid.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "    \n",
    "            # Make prediction and compute loss\n",
    "            pred = model(sid, pid)\n",
    "            loss = F.mse_loss(pred, ratings)\n",
    "    \n",
    "            # Compute gradients w.r.t. loss and take a step in that direction\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    \n",
    "            # Keep track of running loss\n",
    "            total_data += len(sid)\n",
    "            total_loss += len(sid) * loss.item()\n",
    "    \n",
    "        # Evaluate model on validation data\n",
    "        total_val_mse = 0.0\n",
    "        total_val_data = 0\n",
    "        model.eval()\n",
    "        for sid, pid, ratings in valid_loader:\n",
    "            # Move data to GPU\n",
    "            sid = sid.to(device)\n",
    "            pid = pid.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "    \n",
    "            # Clamp predictions in [1,5], since all ground-truth ratings are\n",
    "            pred = model(sid, pid).clamp(1, 5)\n",
    "            mse = F.mse_loss(pred, ratings)\n",
    "    \n",
    "            # Keep track of running metrics\n",
    "            total_val_data += len(sid)\n",
    "            total_val_mse += len(sid) * mse.item()\n",
    "    \n",
    "        print(f\"[Epoch {epoch+1}/{NUM_EPOCHS}] Train loss={total_loss / total_data:.3f}, Valid RMSE={(total_val_mse / total_val_data) ** 0.5:.3f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aab57f9-37f9-4a04-bbf0-2c2352dfbbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "[Epoch 1/1] Train loss=1.087, Valid RMSE=0.911\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "226f942f-2d1c-4ffa-8cdb-f81ee45550f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.911\n"
     ]
    }
   ],
   "source": [
    "pred_fn = lambda sids, pids: model(torch.from_numpy(sids).to(device), torch.from_numpy(pids).to(device)).clamp(1, 5).cpu().numpy()\n",
    "\n",
    "# Evaluate on validation data\n",
    "with torch.no_grad():\n",
    "    val_score = evaluate(valid_df, pred_fn)\n",
    "\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a454f-f40c-439c-a98f-0b6dad45a467",
   "metadata": {},
   "source": [
    "### Test data and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b6b85f8-9b1c-4971-b73d-f6686142aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    make_submission(pred_fn, \"collab-filtering-NCF.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
