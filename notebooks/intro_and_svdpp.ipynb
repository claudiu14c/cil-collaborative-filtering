{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0003cb4-18fb-4ad8-b5d7-24a1626e066c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Collaborative filtering project\n",
    "\n",
    "In this project, the task is to create a paper recommendation system. The system consists of 10,000 scientists and 1,000 papers. Scientists give ratings between 1–5 to the papers that they read. Since not all scientists have read every paper, we only have a limited amount of observations of these ratings. Additionally, each scientist has a wishlist of papers that they would like to read in the future. Your task is to fill in the missing observations using the provided rating and wishlist data, such that we can recommend papers to scientists that we expect them to rate highly.\n",
    "\n",
    "More specifically, there are three data sources:\n",
    " - `train_tbr.csv` containing wishlist data.\n",
    " - `train_ratings.csv` containing observed rating data.\n",
    " - `sample_submission.csv` containing (scientist, paper) pairs that have to be rated for the evaluation of your method.\n",
    "\n",
    "The data is available at `/cluster/courses/cil/collaborative_filtering/data` and an environment has been prepared for you at `/cluster/courses/cil/envs/collaborative_filtering`. You can activate the environment in your shell by running:\n",
    "```bash\n",
    "conda activate /cluster/courses/cil/envs/collaborative_filtering\n",
    "```\n",
    "If you wish to use notebooks on the cluster, you need to set the Environment path to `/cluster/courses/cil/envs/collaborative_filtering/bin` and load the `cuda/12.6` module.\n",
    "\n",
    "**Evaluation**: Your models are evaluated using the root mean-squared error (RMSE) metric. Your grade is determined by a linear interpolation between the easy (grade 4) and hard (grade 6) baselines.\n",
    "\n",
    "**Rules**: You are only allowed to use the data provided in `train_tbr.csv` and `train_ratings.csv` to make your predictions of `sample_submission.csv`. You are not allowed to use external data sources. But, you are allowed to use pre-trained models, as long as they are available publicly. Furthermore, no external API calls are allowed, except for downloading the weights of pre-trained models.\n",
    "\n",
    "**We will verify your code for plagiarism and using solutions from previous years.**\n",
    "\n",
    "[Link to Kaggle competition](https://www.kaggle.com/competitions/ethz-cil-collaborative-filtering-2025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d799b9af-d742-4cac-a937-06102e652812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063fd35-caac-4ced-b13e-b49cfb58d9a2",
   "metadata": {},
   "source": [
    "Make sure that results are reproducible by using a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73627bd-1106-4276-a498-32b44f1b5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5b6d6-37ed-40d1-b651-962c611a22c3",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93bc867-b2d9-4cf7-9bb8-ecb13c663eb3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/cluster/courses/cil/collaborative_filtering/data\"\n",
    "\n",
    "\n",
    "def read_data_df() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reads in data and splits it into training and validation sets with a 75/25 split.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "\n",
    "    # Split sid_pid into sid and pid columns\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "    \n",
    "    # Split into train and validation dataset\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.25)\n",
    "    return train_df, valid_df\n",
    "\n",
    "\n",
    "def read_data_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Returns matrix view of the training data, where columns are scientists (sid) and\n",
    "    rows are papers (pid).\"\"\"\n",
    "\n",
    "    return df.pivot(index=\"sid\", columns=\"pid\", values=\"rating\").values\n",
    "\n",
    "\n",
    "def evaluate(valid_df: pd.DataFrame, pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        valid_df: Validation data, returned from read_data_df for example.\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs their rating predictions.\n",
    "\n",
    "    Outputs: Validation RMSE\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = pred_fn(valid_df[\"sid\"].values, valid_df[\"pid\"].values)\n",
    "    return root_mean_squared_error(valid_df[\"rating\"].values, preds)\n",
    "\n",
    "\n",
    "def make_submission(pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray], filename: os.PathLike):\n",
    "    \"\"\"Makes a submission CSV file that can be submitted to kaggle.\n",
    "\n",
    "    Inputs:\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs a score.\n",
    "        filename: File to save the submission to.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "    # Get sids and pids\n",
    "    sid_pid = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    sids = sid_pid[0]\n",
    "    pids = sid_pid[1]\n",
    "    sids = sids.astype(int).values\n",
    "    pids = pids.astype(int).values\n",
    "    \n",
    "    df[\"rating\"] = pred_fn(sids, pids)\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "092c2bb8-87ed-4821-9f9b-df630ea4fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = read_data_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a5ec452-bf0c-45c0-a9f8-f51bac840685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        rating   sid  pid\n",
      "110268       5   929  783\n",
      "259178       4  2192  558\n",
      "131932       4  1109   64\n",
      "671155       4  5771  410\n",
      "121958       5  1027   57\n"
     ]
    }
   ],
   "source": [
    "print(train_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f712120e-233f-4c3c-bae7-351c9780d41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan ... nan nan nan]\n",
      " [nan  5. nan ... nan nan  3.]\n",
      " [nan  4.  3. ... nan nan nan]\n",
      " ...\n",
      " [nan  3. nan ... nan nan nan]\n",
      " [nan  3. nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(read_data_matrix(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb53ec-a5aa-408c-b6a5-118ffe14c035",
   "metadata": {},
   "source": [
    "## Singular value decomposition\n",
    "\n",
    "For the first method in this introduction, we will make use of the singular value decomposition (SVD) to construct the optimal rank-$k$ approximation (when measuring the Frobenius norm as error), according to the Eckart-Young theorem. Since the matrix needs to be fully observed in order to make use of SVD, we need to impute the missing values. In this case, we impute values with $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3495d53a-ec1e-4692-a19b-6d8aa5fcf88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_values(mat: np.ndarray) -> np.ndarray:\n",
    "    return np.nan_to_num(mat, nan=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a08ecda7-3e44-4d99-894d-6927a12fe54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = read_data_df()\n",
    "train_mat = read_data_matrix(train_df)\n",
    "train_mat = impute_values(train_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f457d-7dd2-448e-8282-792904b67298",
   "metadata": {},
   "source": [
    "### Singular value spectrum\n",
    "\n",
    "In order to assess which rank $k$ to use for the reconstruction matrix, we look at the spectrum of singular values and look for the \"elbow\". In this case, we will use $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f10921bc-224b-46a0-bc0b-67b526c1348a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix shape: (846140, 3)\n",
      "Validation matrix shape: (282047, 3)\n",
      "Train matrix head: \n",
      " <bound method NDFrame.head of          rating   sid  pid\n",
      "663940        4  5708  660\n",
      "93337         3   785  203\n",
      "114265        4   963  754\n",
      "318446        5  2706   62\n",
      "1068983       3  9416  791\n",
      "...         ...   ...  ...\n",
      "79568         3   668  517\n",
      "135553        4  1141   36\n",
      "1101280       5  9729  596\n",
      "1044863       5  9183  173\n",
      "299091        3  2537  668\n",
      "\n",
      "[846140 rows x 3 columns]>\n",
      "Validation matrix head: \n",
      " <bound method NDFrame.head of         rating   sid  pid\n",
      "872090       3  7581    2\n",
      "848396       4  7364  300\n",
      "705686       5  6071   62\n",
      "222418       1  1878  358\n",
      "29179        4   240  905\n",
      "...        ...   ...  ...\n",
      "872114       3  7581  183\n",
      "720226       4  6203  683\n",
      "106367       5   896   63\n",
      "679268       3  5837  315\n",
      "414323       3  3540  222\n",
      "\n",
      "[282047 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train matrix shape: {train_df.shape}\")\n",
    "print(f\"Validation matrix shape: {valid_df.shape}\")\n",
    "print(f\"Train matrix head: \\n {train_df.head}\")\n",
    "print(f\"Validation matrix head: \\n {valid_df.head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d0f8500-fb85-4bf4-a609-eb8a366587d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHHCAYAAACiOWx7AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQk1JREFUeJzt3Xd8FNX+//H3JiEFSKGYhCDNQhNslBgpFiJBY0FRRKICIugVEOSnKBbkKhhEqaJiuYL3CoL4BS+CoJGqEkMRkKIRFQTFJHohWWrant8fmDFLAm4kYWb19Xw89kF25rMzZ06QfTtnzozLGGMEAACAkwqwuwEAAAD+gNAEAADgA0ITAACADwhNAAAAPiA0AQAA+IDQBAAA4ANCEwAAgA8ITQAAAD4gNAEAAPiA0AT8BTRu3Fh9+/a1uxmWyy+/XJdffrndzfhTnNaXAJyD0AQ42JYtW3TzzTerUaNGCg0NVf369XXVVVfphRdesLtp+BuZPXu2Jk+ebHczANsF2d0AAOVbs2aNrrjiCjVs2FADBgxQbGys9uzZo88//1xTpkzRkCFDrNrMzEwFBPD/QKgas2fP1tatWzVs2DC7mwLYitAEONTYsWMVGRmpdevWKSoqymtdTk6O1/uQkJDT2LLTq6ioSB6PR8HBwXY3BT44evSogoODCfH4S+JvNeBQ3333nc4777wygUmSoqOjvd4ffx3OzJkz5XK59Nlnn2n48OE644wzVKNGDd1444365ZdfvD7r8Xg0evRoxcXFqXr16rriiiu0ffv2MtscPXq0XC5XmbaU7GvXrl0nPJaCggKNGjVKbdq0UWRkpGrUqKFOnTppxYoVXnW7du2Sy+XS888/r8mTJ+vss89WSEiItm/fXu52W7VqpSuuuKLMco/Ho/r16+vmm2+2lj3//PO69NJLVadOHYWFhalNmzZ69913T9jmP3vcS5YsUadOnVSjRg2Fh4crOTlZ27Zt+8P9FBYW6p///KfOPfdchYaGqk6dOurYsaPS0tKsmr59+6pmzZr6/vvvlZSUpBo1aiguLk5PPfWUjDFl+mDy5Mk677zzFBoaqpiYGN1zzz3av39/mX0vWbJEl112mcLDwxUREaF27dpp9uzZko5dn7Z48WL98MMPcrlccrlcaty4sSRp5cqVcrlcmjNnjh5//HHVr19f1atXl9vtrlC/NW7cWNdee61Wrlyptm3bKiwsTK1bt9bKlSslSfPnz1fr1q0VGhqqNm3aaOPGjX/Yn0BV4EwT4FCNGjVSenq6tm7dqlatWv2pbQwZMkS1atXSk08+qV27dmny5MkaPHiw5s6da9WMHDlS48eP13XXXaekpCRt3rxZSUlJOnr0aGUditxut15//XXddtttGjBggA4cOKB//etfSkpK0tq1a3XhhRd61c+YMUNHjx7VwIEDFRISotq1a5e73VtvvVWjR49WVlaWYmNjreWffvqp9u7dq169elnLpkyZouuvv14pKSkqKCjQnDlzdMstt2jRokVKTk6ulOP8z3/+oz59+igpKUnPPvusDh8+rJdfflkdO3bUxo0brbBRntGjRys1NVV333232rdvL7fbrfXr1+uLL77QVVddZdUVFxerW7duuuSSSzR+/HgtXbpUTz75pIqKivTUU09Zdffcc49mzpypfv366f7779fOnTs1bdo0bdy4UZ999pmqVasm6ViIueuuu3Teeedp5MiRioqK0saNG7V06VL17t1bjz32mPLy8vTjjz9q0qRJkqSaNWt6tf3pp59WcHCwHnzwQeXn5/+ps4LffvutevfurXvuuUe33367nn/+eV133XWaPn26Hn30Ud13332SpNTUVPXs2ZMhadjDAHCkjz76yAQGBprAwECTkJBgRowYYT788ENTUFBQprZRo0amT58+1vsZM2YYSSYxMdF4PB5r+QMPPGACAwNNbm6uMcaYrKwsExQUZLp37+61vdGjRxtJXtt88sknTXn/ZJTsa+fOndayyy67zFx22WXW+6KiIpOfn+/1uf3795uYmBhz1113Wct27txpJJmIiAiTk5Nz0v4xxpjMzEwjybzwwgtey++77z5Ts2ZNc/jwYWtZ6Z+NMaagoMC0atXKXHnllV7Lj+9LX4/7wIEDJioqygwYMMCrLisry0RGRpZZfrwLLrjAJCcnn7SmT58+RpIZMmSItczj8Zjk5GQTHBxsfvnlF2OMMZ988omRZGbNmuX1+aVLl3otz83NNeHh4SY+Pt4cOXLEq7b035vk5GTTqFGjMu1ZsWKFkWTOOuusMv1bkb8vjRo1MpLMmjVrrGUffvihkWTCwsLMDz/8YC1/5ZVXjCSzYsWKE/QSUHWI6YBDXXXVVUpPT9f111+vzZs3a/z48UpKSlL9+vW1cOFCn7YxcOBAryGSTp06qbi4WD/88IMkadmyZSoqKrL+L75E6YvMK0NgYKB19sHj8Wjfvn0qKipS27Zt9cUXX5Sp79Gjh84444w/3G7Tpk114YUXep05Ky4u1rvvvqvrrrtOYWFh1vLSP+/fv195eXnq1KlTufv/M9LS0pSbm6vbbrtNv/76q/UKDAxUfHx8maHI40VFRWnbtm3asWPHH+5r8ODB1s8ul0uDBw9WQUGBPv74Y0nSvHnzFBkZqauuusqrLW3atFHNmjWttqSlpenAgQN65JFHFBoa6rWP8obWTqRPnz5e/ftntGzZUgkJCdb7+Ph4SdKVV16phg0blln+/fffn9L+gD+D4TnAwdq1a6f58+eroKBAmzdv1oIFCzRp0iTdfPPN2rRpk1q2bHnSz5f+spGkWrVqSZJ1XUtJeDrnnHO86mrXrm3VVpY333xTEyZM0Ndff63CwkJreZMmTcrUlrfsRG699VY9+uij+umnn1S/fn2tXLlSOTk5uvXWW73qFi1apDFjxmjTpk3Kz8+3llckHJxMSdi58sory10fERFx0s8/9dRTuuGGG9S0aVO1atVK3bp10x133KHzzz/fqy4gIEBnnXWW17KmTZtKknWd0I4dO5SXl1fm2rcSJRMJvvvuO0n608O/JSry+zqR4/+uRkZGSpIaNGhQ7vLyrs0CqhqhCfADwcHBateundq1a6emTZuqX79+mjdvnp588smTfi4wMLDc5ea4i4Z9caJwUVxc/Ieffeutt9S3b191795dDz30kKKjoxUYGKjU1FTri7u0ipy1uPXWWzVy5EjNmzdPw4YN0zvvvKPIyEh169bNqvnkk090/fXXq3PnznrppZdUr149VatWTTNmzLAueD4RX4/b4/FIOnZdU+nrq0oEBZ38n9vOnTvru+++03//+1999NFHev311zVp0iRNnz5dd99990k/ezyPx6Po6GjNmjWr3PW+nMWriPJ+XxX9+3Kiv6uV+XcYOFWEJsDPtG3bVpL0888/n/K2GjVqJOnYRbilzxb873//K/N/8iVnnnJzc71m9JWcrTqZd999V2eddZbmz5/v9WX6R6HPF02aNFH79u01d+5cDR48WPPnz1f37t29bsPwf//3fwoNDdWHH37otXzGjBl/uH1fj/vss8+WdGxmY2Ji4p86ltq1a6tfv37q16+fDh48qM6dO2v06NFeocnj8ej777+3zi5J0jfffCNJ1oXmZ599tj7++GN16NDhpAG0pM1bt24tc7axtD9zNu5U/r4ATsU1TYBDrVixotz/m/7ggw8kSc2aNTvlfXTp0kVBQUF6+eWXvZZPmzatTG3JF+zq1autZYcOHdKbb775h/spOVtQ+ngyMjKUnp7+p9p9vFtvvVWff/653njjDf36669lhuYCAwPlcrm8znLs2rVL77333h9u29fjTkpKUkREhJ555hmv4ccSx9/q4Xj/+9//vN7XrFlT55xzjtdQYonSvx9jjKZNm6Zq1aqpS5cukqSePXuquLhYTz/9dJnPFhUVKTc3V5LUtWtXhYeHKzU1tcxsydK/qxo1aigvL++k7T/eqfx9AZyKM02AQw0ZMkSHDx/WjTfeqObNm6ugoEBr1qzR3Llz1bhxY/Xr1++U9xETE6OhQ4dqwoQJuv7669WtWzdt3rxZS5YsUd26db3OMHTt2lUNGzZU//799dBDDykwMFBvvPGGzjjjDO3evfuk+7n22ms1f/583XjjjUpOTtbOnTs1ffp0tWzZUgcPHjzl4+jZs6cefPBBPfjgg6pdu3aZMz3JycmaOHGiunXrpt69eysnJ0cvvviizjnnHH355Zcn3bavxx0REaGXX35Zd9xxhy6++GL16tXLqlm8eLE6dOhQbhgt0bJlS11++eVq06aNateurfXr1+vdd9/1uuhbkkJDQ7V06VL16dNH8fHxWrJkiRYvXqxHH33UGna77LLLdM899yg1NVWbNm1S165dVa1aNe3YsUPz5s3TlClTdPPNNysiIkKTJk3S3XffrXbt2ql3796qVauWNm/erMOHD1sBp02bNpo7d66GDx+udu3aqWbNmrruuusqpd8Av2Ln1D0AJ7ZkyRJz1113mebNm5uaNWua4OBgc84555ghQ4aY7Oxsr9oT3XJg3bp1XnUlU8RLT9cuKioyTzzxhImNjTVhYWHmyiuvNF999ZWpU6eOuffee70+v2HDBhMfH2+Cg4NNw4YNzcSJE3265YDH4zHPPPOMadSokQkJCTEXXXSRWbRokenTp4/XVPaSWw4899xzFe6vDh06GEnm7rvvLnf9v/71L3PuueeakJAQ07x5czNjxoxyp8Uf35cVOW5jjvVxUlKSiYyMNKGhoebss882ffv2NevXrz9p+8eMGWPat29voqKiTFhYmGnevLkZO3as1y0m+vTpY2rUqGG+++4707VrV1O9enUTExNjnnzySVNcXFxmm6+++qpp06aNCQsLM+Hh4aZ169ZmxIgRZu/evV51CxcuNJdeeqkJCwszERERpn379ubtt9+21h88eND07t3bREVFGUnW76zk79O8efPKPSZf+61Ro0bl3m5Bkhk0aJDXslP5OwKcKpcxXE0HwFtubq5q1aqlMWPG6LHHHrO7OfhN37599e6771bK2TkAFcc1TcDf3JEjR8osK3mi/eWXX356GwMADsY1TcDf3Ny5czVz5kxdc801qlmzpj799FO9/fbb6tq1qzp06GB38wDAMQhNwN/c+eefr6CgII0fP15ut9u6OHzMmDF2Nw0AHIVrmgAAAHzANU0AAAA+IDQBAAD4gGuaKonH49HevXsVHh5eaQ8ABQAAVcsYowMHDiguLk4BASc/l0RoqiR79+4t8zRuAADgH/bs2aMzzzzzpDWEpkoSHh4u6VinR0RE2NwaAADgC7fbrQYNGljf4ydDaKokJUNyERERhCYAAPyML5fW2Hoh+OrVq3XdddcpLi5OLperzBPHjTEaNWqU6tWrp7CwMCUmJmrHjh1eNfv27VNKSooiIiIUFRWl/v37l3nEwJdffqlOnTopNDRUDRo00Pjx48u0Zd68eWrevLlCQ0PVunVr60nyAAAAks2h6dChQ7rgggv04osvlrt+/Pjxmjp1qqZPn66MjAzVqFFDSUlJOnr0qFWTkpKibdu2KS0tTYsWLdLq1as1cOBAa73b7VbXrl3VqFEjbdiwQc8995xGjx6tV1991apZs2aNbrvtNvXv318bN25U9+7d1b17d23durXqDh4AAPgXO58WXJoks2DBAuu9x+MxsbGxXk+yzs3NNSEhIdbTt7dv317mSe5LliwxLpfL/PTTT8YYY1566SVTq1Ytk5+fb9U8/PDDplmzZtb7nj17lnnCdnx8vLnnnnt8bn9eXp6RZPLy8nz+DAAAsFdFvr8de5+mnTt3KisrS4mJidayyMhIxcfHKz09XZKUnp6uqKgotW3b1qpJTExUQECAMjIyrJrOnTsrODjYqklKSlJmZqb2799v1ZTeT0lNyX7Kk5+fL7fb7fUCAAB/XY4NTVlZWZKkmJgYr+UxMTHWuqysLEVHR3utDwoKUu3atb1qyttG6X2cqKZkfXlSU1MVGRlpvbjdAAAAf22ODU1ON3LkSOXl5VmvPXv22N0kAABQhRwbmmJjYyVJ2dnZXsuzs7OtdbGxscrJyfFaX1RUpH379nnVlLeN0vs4UU3J+vKEhIRYtxfgNgMAAPz1OTY0NWnSRLGxsVq2bJm1zO12KyMjQwkJCZKkhIQE5ebmasOGDVbN8uXL5fF4FB8fb9WsXr1ahYWFVk1aWpqaNWumWrVqWTWl91NSU7IfAAAAW0PTwYMHtWnTJm3atEnSsYu/N23apN27d8vlcmnYsGEaM2aMFi5cqC1btujOO+9UXFycunfvLklq0aKFunXrpgEDBmjt2rX67LPPNHjwYPXq1UtxcXGSpN69eys4OFj9+/fXtm3bNHfuXE2ZMkXDhw+32jF06FAtXbpUEyZM0Ndff63Ro0dr/fr1Gjx48OnuEgAA4FSnYTbfCa1YscJIKvPq06ePMebYbQeeeOIJExMTY0JCQkyXLl1MZmam1zb+97//mdtuu83UrFnTREREmH79+pkDBw541WzevNl07NjRhISEmPr165tx48aVacs777xjmjZtaoKDg815551nFi9eXKFj4ZYDAAD4n4p8f7uMMcbGzPaX4Xa7FRkZqby8PK5vAgDAT1Tk+9ux1zQBAAA4CQ/sdbjDBUXad6hAIUGBOiM8xO7mAADwt8WZJof7+KscdXx2hYbO2Wh3UwAA+FsjNAEAAPiA0OQnuFwfAAB7EZoczmV3AwAAgCRCEwAAgE8ITX7CiPE5AADsRGhyOBfjcwAAOAKhCQAAwAeEJj/B7DkAAOxFaHI4F/PnAABwBEITAACADwhNfoLROQAA7EVocjhmzwEA4AyEJgAAAB8QmvwF43MAANiK0ORwjM4BAOAMhCYAAAAfEJr8BM+eAwDAXoQmh2P2HAAAzkBo8hM8RgUAAHsRmhyPU00AADgBoQkAAMAHhCY/wegcAAD2IjQ5HBeCAwDgDIQmAAAAHxCa/IRh+hwAALYiNDkco3MAADgDoQkAAMAHhCY/weAcAAD2IjQ5nIvpcwAAOAKhCQAAwAeEJj/B5DkAAOxFaHI4BucAAHAGQhMAAIAPCE1+gtE5AADsRWhyOCbPAQDgDIQmAAAAHxCa/AXT5wAAsBWhyeEYngMAwBkITQAAAD4gNPkJBucAALAXocnhXNzeEgAARyA0+QmuAwcAwF6EJqfjRBMAAI5AaAIAAPABoclPGC4FBwDAVoQmh2N0DgAAZyA0AQAA+IDQ5CeYPQcAgL0ITQ7n4jkqAAA4AqEJAADAB4QmP8HwHAAA9iI0ORyDcwAAOAOhCQAAwAeEJj/B6BwAAPYiNDkck+cAAHAGR4em4uJiPfHEE2rSpInCwsJ09tln6+mnn5YpdVW0MUajRo1SvXr1FBYWpsTERO3YscNrO/v27VNKSooiIiIUFRWl/v376+DBg141X375pTp16qTQ0FA1aNBA48ePPy3HCAAA/IOjQ9Ozzz6rl19+WdOmTdNXX32lZ599VuPHj9cLL7xg1YwfP15Tp07V9OnTlZGRoRo1aigpKUlHjx61alJSUrRt2zalpaVp0aJFWr16tQYOHGitd7vd6tq1qxo1aqQNGzboueee0+jRo/Xqq6+e1uM9GcP0OQAAbBVkdwNOZs2aNbrhhhuUnJwsSWrcuLHefvttrV27VtKxIDF58mQ9/vjjuuGGGyRJ//73vxUTE6P33ntPvXr10ldffaWlS5dq3bp1atu2rSTphRde0DXXXKPnn39ecXFxmjVrlgoKCvTGG28oODhY5513njZt2qSJEyd6hSs7uJg/BwCAIzj6TNOll16qZcuW6ZtvvpEkbd68WZ9++qmuvvpqSdLOnTuVlZWlxMRE6zORkZGKj49Xenq6JCk9PV1RUVFWYJKkxMREBQQEKCMjw6rp3LmzgoODrZqkpCRlZmZq//795bYtPz9fbrfb6wUAAP66HH2m6ZFHHpHb7Vbz5s0VGBio4uJijR07VikpKZKkrKwsSVJMTIzX52JiYqx1WVlZio6O9lofFBSk2rVre9U0adKkzDZK1tWqVatM21JTU/XPf/6zEo4SAAD4A0efaXrnnXc0a9YszZ49W1988YXefPNNPf/883rzzTftbppGjhypvLw867Vnz54q2Q+z5wAAcAZHn2l66KGH9Mgjj6hXr16SpNatW+uHH35Qamqq+vTpo9jYWElSdna26tWrZ30uOztbF154oSQpNjZWOTk5XtstKirSvn37rM/HxsYqOzvbq6bkfUnN8UJCQhQSEnLqBwkAAPyCo880HT58WAEB3k0MDAyUx+ORJDVp0kSxsbFatmyZtd7tdisjI0MJCQmSpISEBOXm5mrDhg1WzfLly+XxeBQfH2/VrF69WoWFhVZNWlqamjVrVu7QnB2YPAcAgL0cHZquu+46jR07VosXL9auXbu0YMECTZw4UTfeeKMkyeVyadiwYRozZowWLlyoLVu26M4771RcXJy6d+8uSWrRooW6deumAQMGaO3atfrss880ePBg9erVS3FxcZKk3r17Kzg4WP3799e2bds0d+5cTZkyRcOHD7fr0C2MzgEA4AyOHp574YUX9MQTT+i+++5TTk6O4uLidM8992jUqFFWzYgRI3To0CENHDhQubm56tixo5YuXarQ0FCrZtasWRo8eLC6dOmigIAA9ejRQ1OnTrXWR0ZG6qOPPtKgQYPUpk0b1a1bV6NGjbL9dgOlGR6kAgCArVyGuyZWCrfbrcjISOXl5SkiIqLStrvmu1/V+7UMNY2pqY8euKzStgsAACr2/e3o4TkAAACnIDT5Cc4HAgBgL0KTw/EYFQAAnIHQBAAA4ANCk59gdA4AAHsRmhyOx6gAAOAMhCYAAAAfEJr8BLfTAgDAXoQmh2N0DgAAZyA0AQAA+IDQ5CcYnAMAwF6EJodzMX0OAABHIDQBAAD4gNDkLxifAwDAVoQmh2N0DgAAZyA0AQAA+IDQ5CcYnQMAwF6EJodjdA4AAGcgNAEAAPiA0OQnePYcAAD2IjQ5HLPnAABwBkKTn+A8EwAA9iI0OR6nmgAAcAJCEwAAgA8ITX6C68ABALAXocnhuBAcAABnIDQBAAD4gNDkJwzz5wAAsBWhyeEYnQMAwBkITQAAAD4gNPkJZs8BAGAvQpPDuZg+BwCAIxCaAAAAfEBo8hMMzwEAYC9Ck8MxOAcAgDMQmgAAAHxAaAIAAPABocnhmDwHAIAzEJoAAAB8QGjyE4bpcwAA2IrQ5HAu5s8BAOAIhCYAAAAfEJr8BINzAADYi9DkcMyeAwDAGQhNfoLrwAEAsBehCQAAwAeEJgAAAB8QmvyE4VJwAABsRWhyOC4EBwDAGQhNAAAAPiA0+QlmzwEAYC9Ck8PxGBUAAJyB0AQAAOADQpOfYHQOAAB7EZocjtlzAAA4A6EJAADAB4QmP8HsOQAA7EVocjiG5wAAcAbHh6affvpJt99+u+rUqaOwsDC1bt1a69evt9YbYzRq1CjVq1dPYWFhSkxM1I4dO7y2sW/fPqWkpCgiIkJRUVHq37+/Dh486FXz5ZdfqlOnTgoNDVWDBg00fvz403J8AADAPzg6NO3fv18dOnRQtWrVtGTJEm3fvl0TJkxQrVq1rJrx48dr6tSpmj59ujIyMlSjRg0lJSXp6NGjVk1KSoq2bdumtLQ0LVq0SKtXr9bAgQOt9W63W127dlWjRo20YcMGPffccxo9erReffXV03q8J8f4HAAAtjIO9vDDD5uOHTuecL3H4zGxsbHmueees5bl5uaakJAQ8/bbbxtjjNm+fbuRZNatW2fVLFmyxLhcLvPTTz8ZY4x56aWXTK1atUx+fr7Xvps1a+ZzW/Py8owkk5eX5/NnfPH1z27T6OFFps3TH1XqdgEAQMW+vx19pmnhwoVq27atbrnlFkVHR+uiiy7Sa6+9Zq3fuXOnsrKylJiYaC2LjIxUfHy80tPTJUnp6emKiopS27ZtrZrExEQFBAQoIyPDquncubOCg4OtmqSkJGVmZmr//v3lti0/P19ut9vrBQAA/rocHZq+//57vfzyyzr33HP14Ycf6h//+Ifuv/9+vfnmm5KkrKwsSVJMTIzX52JiYqx1WVlZio6O9lofFBSk2rVre9WUt43S+zheamqqIiMjrVeDBg1O8WhPjtlzAADYy9GhyePx6OKLL9Yzzzyjiy66SAMHDtSAAQM0ffp0u5umkSNHKi8vz3rt2bOnSvbD7DkAAJzB0aGpXr16atmypdeyFi1aaPfu3ZKk2NhYSVJ2drZXTXZ2trUuNjZWOTk5XuuLioq0b98+r5rytlF6H8cLCQlRRESE16sqcaIJAAB7OTo0dejQQZmZmV7LvvnmGzVq1EiS1KRJE8XGxmrZsmXWerfbrYyMDCUkJEiSEhISlJubqw0bNlg1y5cvl8fjUXx8vFWzevVqFRYWWjVpaWlq1qyZ10w9O3CiCQAAZ3B0aHrggQf0+eef65lnntG3336r2bNn69VXX9WgQYMkSS6XS8OGDdOYMWO0cOFCbdmyRXfeeafi4uLUvXt3ScfOTHXr1k0DBgzQ2rVr9dlnn2nw4MHq1auX4uLiJEm9e/dWcHCw+vfvr23btmnu3LmaMmWKhg8fbtehAwAAhwmyuwEn065dOy1YsEAjR47UU089pSZNmmjy5MlKSUmxakaMGKFDhw5p4MCBys3NVceOHbV06VKFhoZaNbNmzdLgwYPVpUsXBQQEqEePHpo6daq1PjIyUh999JEGDRqkNm3aqG7duho1apTXvZzsZrgSHAAAW7kM38aVwu12KzIyUnl5eZV6fdO3OQeUOHG1alWvpo2julbadgEAQMW+vx09PAcAAOAUhCY/welAAADsRWhyPObPAQDgBIQmAAAAHxCa/ASX6wMAYC9Ck8PxGBUAAJyB0AQAAOADQpOf4HZaAADYi9DkcIzOAQDgDIQmAAAAHxCa/ASDcwAA2IvQ5HAups8BAOAIhCYAAAAfEJr8BeNzAADYitDkcAzOAQDgDIQmAAAAH/yp0FRUVKSPP/5Yr7zyig4cOCBJ2rt3rw4ePFipjcPvGJ0DAMBeQRX9wA8//KBu3bpp9+7dys/P11VXXaXw8HA9++yzys/P1/Tp06uinX9bTJ4DAMAZKnymaejQoWrbtq3279+vsLAwa/mNN96oZcuWVWrj8DseowIAgL0qfKbpk08+0Zo1axQcHOy1vHHjxvrpp58qrWE4xsWl4AAAOEKFzzR5PB4VFxeXWf7jjz8qPDy8UhoFAADgNBUOTV27dtXkyZOt9y6XSwcPHtSTTz6pa665pjLbhlIYnAMAwF4VHp6bMGGCkpKS1LJlSx09elS9e/fWjh07VLduXb399ttV0ca/NS4EBwDAGSocms4880xt3rxZc+bM0ZdffqmDBw+qf//+SklJ8bowHAAA4K+kwqFJkoKCgnT77bdXdltwEkyeAwDAXhUOTf/+979Puv7OO+/8040BAABwqgqHpqFDh3q9Lyws1OHDhxUcHKzq1asTmgAAwF9ShWfP7d+/3+t18OBBZWZmqmPHjlwIXoUM8+cAALBVpTyw99xzz9W4cePKnIXCqWP2HAAAzlApoUk6dnH43r17K2tzAAAAjlLha5oWLlzo9d4Yo59//lnTpk1Thw4dKq1h8MbsOQAA7FXh0NS9e3ev9y6XS2eccYauvPJKTZgwobLahd+4GJ8DAMARKhyaPB5PVbQDAADA0SrtmiZULUbnAACwl09nmoYPH+7zBidOnPinG4OyGJwDAMAZfApNGzdu9GljXH8DAAD+qnwKTStWrKjqduCPMD4HAICtuKbJ4Th5BwCAM1R49pwkrV+/Xu+88452796tgoICr3Xz58+vlIYBAAA4SYXPNM2ZM0eXXnqpvvrqKy1YsECFhYXatm2bli9frsjIyKpoI8Sz5wAAsFuFQ9MzzzyjSZMm6f3331dwcLCmTJmir7/+Wj179lTDhg2roo1/ay7mzwEA4AgVDk3fffedkpOTJUnBwcE6dOiQXC6XHnjgAb366quV3kAcw2NUAACwV4VDU61atXTgwAFJUv369bV161ZJUm5urg4fPly5rQMXggMA4BA+h6aScNS5c2elpaVJkm655RYNHTpUAwYM0G233aYuXbpUTSsBAABs5vPsufPPP1/t2rVT9+7ddcstt0iSHnvsMVWrVk1r1qxRjx499Pjjj1dZQ//uGJ0DAMBePoemVatWacaMGUpNTdXYsWPVo0cP3X333XrkkUeqsn1/e4zOAQDgDD4Pz3Xq1ElvvPGGfv75Z73wwgvatWuXLrvsMjVt2lTPPvussrKyqrKdAAAAtqrwheA1atRQv379tGrVKn3zzTe65ZZb9OKLL6phw4a6/vrrq6KNkGSYPgcAgK1O6TEq55xzjh599FE9/vjjCg8P1+LFiyurXSjB+BwAAI7wpx6jIkmrV6/WG2+8of/7v/9TQECAevbsqf79+1dm2wAAAByjQqFp7969mjlzpmbOnKlvv/1Wl156qaZOnaqePXuqRo0aVdVGiNlzAADYzefQdPXVV+vjjz9W3bp1deedd+quu+5Ss2bNqrJtEI9RAQDAKXwOTdWqVdO7776ra6+9VoGBgVXZJgAAAMfxOTQtXLiwKtuBP8DkOQAA7HVKs+dQ9Xj2HAAAzkBoAgAA8AGhCQAAwAeEJodjdA4AAGfwq9A0btw4uVwuDRs2zFp29OhRDRo0SHXq1FHNmjXVo0cPZWdne31u9+7dSk5OVvXq1RUdHa2HHnpIRUVFXjUrV67UxRdfrJCQEJ1zzjmaOXPmaTgiAADgL/wmNK1bt06vvPKKzj//fK/lDzzwgN5//33NmzdPq1at0t69e3XTTTdZ64uLi5WcnKyCggKtWbNGb775pmbOnKlRo0ZZNTt37lRycrKuuOIKbdq0ScOGDdPdd9+tDz/88LQdny94/hwAAPbxi9B08OBBpaSk6LXXXlOtWrWs5Xl5efrXv/6liRMn6sorr1SbNm00Y8YMrVmzRp9//rkk6aOPPtL27dv11ltv6cILL9TVV1+tp59+Wi+++KIKCgokSdOnT1eTJk00YcIEtWjRQoMHD9bNN9+sSZMm2XK8pbmYPgcAgCP4RWgaNGiQkpOTlZiY6LV8w4YNKiws9FrevHlzNWzYUOnp6ZKk9PR0tW7dWjExMVZNUlKS3G63tm3bZtUcv+2kpCRrGwAAAH/6gb2ny5w5c/TFF19o3bp1ZdZlZWUpODhYUVFRXstjYmKUlZVl1ZQOTCXrS9adrMbtduvIkSMKCwsrs+/8/Hzl5+db791ud8UProKM4b5NAADYxdFnmvbs2aOhQ4dq1qxZCg0Ntbs5XlJTUxUZGWm9GjRoUCX7ISMBAOAMjg5NGzZsUE5Oji6++GIFBQUpKChIq1at0tSpUxUUFKSYmBgVFBQoNzfX63PZ2dmKjY2VJMXGxpaZTVfy/o9qIiIiyj3LJEkjR45UXl6e9dqzZ09lHPJJcRk4AAD2cXRo6tKli7Zs2aJNmzZZr7Zt2yolJcX6uVq1alq2bJn1mczMTO3evVsJCQmSpISEBG3ZskU5OTlWTVpamiIiItSyZUurpvQ2SmpKtlGekJAQRUREeL2qAsNxAAA4g6OvaQoPD1erVq28ltWoUUN16tSxlvfv31/Dhw9X7dq1FRERoSFDhighIUGXXHKJJKlr165q2bKl7rjjDo0fP15ZWVl6/PHHNWjQIIWEhEiS7r33Xk2bNk0jRozQXXfdpeXLl+udd97R4sWLT+8BAwAAx3J0aPLFpEmTFBAQoB49eig/P19JSUl66aWXrPWBgYFatGiR/vGPfyghIUE1atRQnz599NRTT1k1TZo00eLFi/XAAw9oypQpOvPMM/X6668rKSnJjkM6oWP3aeLUEwAAdnAZ7phYKdxutyIjI5WXl1epQ3V5hwt1wVMfSZK+HXu1ggIdPaIKAIBfqcj3N9/AAAAAPiA0+RFOCQIAYB9Ck9NxCRMAAI5AaAIAAPABocmPcMk+AAD2ITQ5HDe3BADAGQhNAAAAPiA0+RHD/DkAAGxDaHI4RucAAHAGQhMAAIAPCE1+hNlzAADYh9DkcC6mzwEA4AiEJgAAAB8QmgAAAHxAaHI4BucAAHAGQhMAAIAPCE1+hNlzAADYh9DkcEyeAwDAGQhNfoTHqAAAYB9Ck8O5uBQcAABHIDQBAAD4gNDkR7gQHAAA+xCaHI4LwQEAcAZCEwAAgA8ITX6E0TkAAOxDaAIAAPABoQkAAMAHhCY/Ypg+BwCAbQhNDsfsOQAAnIHQBAAA4ANCkx9hcA4AAPsQmhyOZ88BAOAMhCYAAAAfEJr8CJPnAACwD6HJ4Zg9BwCAMxCaAAAAfEBo8icMzwEAYBtCk8MxOgcAgDMQmvyI4VQTAAC2ITQ5nIsrwQEAcARCEwAAgA8ITX6E+zQBAGAfQpPDMTgHAIAzEJoAAAB8QGjyI4zOAQBgH0KTwzF5DgAAZyA0AQAA+IDQ5EcM0+cAALANocnhuLklAADOQGgCAADwAaHJjzA4BwCAfQhNAAAAPiA0AQAA+IDQ5EeYPAcAgH0ITX6ACXQAANiP0AQAAOADQpMfMcyfAwDANoQmP8DoHAAA9iM0AQAA+MDRoSk1NVXt2rVTeHi4oqOj1b17d2VmZnrVHD16VIMGDVKdOnVUs2ZN9ejRQ9nZ2V41u3fvVnJysqpXr67o6Gg99NBDKioq8qpZuXKlLr74YoWEhOicc87RzJkzq/rwKo7ROQAAbOPo0LRq1SoNGjRIn3/+udLS0lRYWKiuXbvq0KFDVs0DDzyg999/X/PmzdOqVau0d+9e3XTTTdb64uJiJScnq6CgQGvWrNGbb76pmTNnatSoUVbNzp07lZycrCuuuEKbNm3SsGHDdPfdd+vDDz88rcd7Ijx/DgAA+7mM8Z+7//zyyy+Kjo7WqlWr1LlzZ+Xl5emMM87Q7NmzdfPNN0uSvv76a7Vo0ULp6em65JJLtGTJEl177bXau3evYmJiJEnTp0/Xww8/rF9++UXBwcF6+OGHtXjxYm3dutXaV69evZSbm6ulS5f61Da3263IyEjl5eUpIiKiUo/77Ec/ULHHKOPRLoqJCK3UbQMA8HdWke9vR59pOl5eXp4kqXbt2pKkDRs2qLCwUImJiVZN8+bN1bBhQ6Wnp0uS0tPT1bp1ayswSVJSUpLcbre2bdtm1ZTeRklNyTbKk5+fL7fb7fWqKpxnAgDAfn4Tmjwej4YNG6YOHTqoVatWkqSsrCwFBwcrKirKqzYmJkZZWVlWTenAVLK+ZN3Jatxut44cOVJue1JTUxUZGWm9GjRocMrHCAAAnMtvQtOgQYO0detWzZkzx+6mSJJGjhypvLw867Vnz54q36f/DKQCAPDXE2R3A3wxePBgLVq0SKtXr9aZZ55pLY+NjVVBQYFyc3O9zjZlZ2crNjbWqlm7dq3X9kpm15WuOX7GXXZ2tiIiIhQWFlZum0JCQhQSEnLKx+YLrgMHAMB+jj7TZIzR4MGDtWDBAi1fvlxNmjTxWt+mTRtVq1ZNy5Yts5ZlZmZq9+7dSkhIkCQlJCRoy5YtysnJsWrS0tIUERGhli1bWjWlt1FSU7INAAAAR59pGjRokGbPnq3//ve/Cg8Pt65BioyMVFhYmCIjI9W/f38NHz5ctWvXVkREhIYMGaKEhARdcsklkqSuXbuqZcuWuuOOOzR+/HhlZWXp8ccf16BBg6wzRffee6+mTZumESNG6K677tLy5cv1zjvvaPHixbYde3l4jAoAAPZx9Jmml19+WXl5ebr88stVr1496zV37lyrZtKkSbr22mvVo0cPde7cWbGxsZo/f761PjAwUIsWLVJgYKASEhJ0++23684779RTTz1l1TRp0kSLFy9WWlqaLrjgAk2YMEGvv/66kpKSTuvxnoiL+XMAANjOr+7T5GRVeZ+mpo8tUUGxR+kjr1S9yPKvsQIAABX3l71P098d8RYAAPsQmvwBo3MAANiO0AQAAOADQpMfYXQOAAD7EJr8AKNzAADYj9AEAADgA0KTH+HuEAAA2IfQ5Ad49hwAAPYjNAEAAPiA0ORHGJ0DAMA+hCY/wLPnAACwH6EJAADAB4QmAAAAHxCa/ACz5wAAsB+hyY9wITgAAPYhNPkBTjQBAGA/QhMAAIAPCE1+xIjxOQAA7EJo8gMurgQHAMB2hCYAAAAfEJr8CLPnAACwD6HJDzA4BwCA/QhNAAAAPiA0+RFG5wAAsA+hyR8wPgcAgO0ITQAAAD4gNPkRw/Q5AABsQ2jyA4zOAQBgP0ITAACADwhNfoTBOQAA7ENo8gM8ew4AAPsRmgAAAHxAaPIjTJ4DAMA+hCY/wOgcAAD2IzQBAAD4gNDkVxifAwDALoQmP8DoHAAA9iM0+REuBAcAwD6EJj/AfZoAALAfoQkAAMAHhCY/wugcAAD2ITT5AQbnAACwH6EJAADAB4QmP8LsOQAA7ENo8gNMngMAwH6EJgAAAB8QmvyIYf4cAAC2ITT5BcbnAACwG6EJAADAB4QmP8LsOQAA7ENo8gPMngMAwH6EJgAAAB8QmvwIw3MAANiH0OQHGJ0DAMB+hCYAAAAfEJr8CDe3BADAPoQmP8DsOQAA7Edo8iNf/pgnw9XgAADYgtB0nBdffFGNGzdWaGio4uPjtXbtWrubpFZxkZKkkfO3qPNzKzRuydd6f/NerczM0Re79+vbnAPKcR/V0cJiQhUAAFXEZfiWtcydO1d33nmnpk+frvj4eE2ePFnz5s1TZmamoqOjT/pZt9utyMhI5eXlKSIiolLb9cuBfI1ZvF0fbcvWkcLik9YGBbhULTBAQYHH/gwMcKlagEtBgQEKCnApKNCloICA3/4svTxA1QJcx+p/+/yxzwYoMPAE2/htWWCAFOByyeVyKcB17OcAl35771JgwO8/l6x3/fZnYIB3ben1x9b9Xvv7z5L0+3KXfv+M5L19l+vY7EOXy7u+5GdJCgjwXla6vvS2T7qNE9QDAJytIt/fhKZS4uPj1a5dO02bNk2S5PF41KBBAw0ZMkSPPPLIST9blaGpxJGCYi3/Okdp27OU5T4q95EiHcgvPPbn0UJ5+E06jldos967rPtIeC0rp76kqHSQ+/1zv9e4SgXJ0stcZZa5StWfuMZVshOVbf9veyq1DZf3sRxX4yq1w9+P13tfpfvg9+M9rl+Oe+/V1hP0Xel+Ob6/j++Dkx+v93H+3s7j2nNc3x/f16pIfal96g/qfu8j39pY+ndUZlvHbcPXff5RG0+0T6lsu0/6d7YCbTx+n8f/jsseS0XbeOJ9lve79mWff9jGcn9n5bfbaxun0sZy++ME+6ysfjnBPqsHB6l2jWBVpop8fwdV6p79WEFBgTZs2KCRI0daywICApSYmKj09PQy9fn5+crPz7feu93uKm9jWHCgks+vp+Tz65VZZ4zRoYJiHThaqMIioyKPR0Ueo6LiYz8XFhsVe4yKij0q9BgV/7asZH3Rb+sLf/u56Lfastv47XMez7H6YiOPMTJG8hgjz29/GnNsex5zrG2eUut/X1f+ek+Zdb8vN/q93ujYfo3xXuYxJTcC9d6++a2dx2/DYySVtLvUuspQsj/vO5OSbgHgz7j+gjhNve0i2/ZPaPrNr7/+quLiYsXExHgtj4mJ0ddff12mPjU1Vf/85z9PV/P+kMvlUs2QINUM4VdaWUqHLI/18+9BrSRklaxTOcuMFcRKffa47f++vxPUqHTmOn69d5t+qzil7VrbOa7G6FhBufs+brs6ri0V3nc521V5633Z90m2q9K/n5J9qOx2S/fl8X1sTrLu98+VX19yG5Hj15W0qyL7VJl1vu3Tq40V2KdKr/uDenNc48r8zo7rg9K/C+/P+b5PHXecvu5TZdb5tk+d5Pd5on3quOM8+T7Lb6Mv+1Q52/BlnzpB/Yn3+fu7k/0+j/83xpc2llRWC7T3Umy+Yf+kkSNHavjw4dZ7t9utBg0a2NgiVLbSw2GBcp28GADwl0do+k3dunUVGBio7Oxsr+XZ2dmKjY0tUx8SEqKQkJDT1TwAAGAzbjnwm+DgYLVp00bLli2zlnk8Hi1btkwJCQk2tgwAADgBZ5pKGT58uPr06aO2bduqffv2mjx5sg4dOqR+/frZ3TQAAGAzQlMpt956q3755ReNGjVKWVlZuvDCC7V06dIyF4cDAIC/H+7TVElOx32aAABA5arI9zfXNAEAAPiA0AQAAOADQhMAAIAPCE0AAAA+IDQBAAD4gNAEAADgA0ITAACADwhNAAAAPiA0AQAA+IDHqFSSkhuru91um1sCAAB8VfK97csDUghNleTAgQOSpAYNGtjcEgAAUFEHDhxQZGTkSWt49lwl8Xg82rt3r8LDw+VyuSp12263Ww0aNNCePXt4rl0Vop9PD/r59KGvTw/6+fSoqn42xujAgQOKi4tTQMDJr1riTFMlCQgI0Jlnnlml+4iIiOA/yNOAfj496OfTh74+Pejn06Mq+vmPzjCV4EJwAAAAHxCaAAAAfEBo8gMhISF68sknFRISYndT/tLo59ODfj596OvTg34+PZzQz1wIDgAA4APONAEAAPiA0AQAAOADQhMAAIAPCE0AAAA+IDQ53IsvvqjGjRsrNDRU8fHxWrt2rd1N8iupqalq166dwsPDFR0dre7duyszM9Or5ujRoxo0aJDq1KmjmjVrqkePHsrOzvaq2b17t5KTk1W9enVFR0froYceUlFR0ek8FL8ybtw4uVwuDRs2zFpGP1eOn376Sbfffrvq1KmjsLAwtW7dWuvXr7fWG2M0atQo1atXT2FhYUpMTNSOHTu8trFv3z6lpKQoIiJCUVFR6t+/vw4ePHi6D8XRiouL9cQTT6hJkyYKCwvT2Wefraefftrr+WT0dcWtXr1a1113neLi4uRyufTee+95ra+sPv3yyy/VqVMnhYaGqkGDBho/fnzlHICBY82ZM8cEBwebN954w2zbts0MGDDAREVFmezsbLub5jeSkpLMjBkzzNatW82mTZvMNddcYxo2bGgOHjxo1dx7772mQYMGZtmyZWb9+vXmkksuMZdeeqm1vqioyLRq1cokJiaajRs3mg8++MDUrVvXjBw50o5Dcry1a9eaxo0bm/PPP98MHTrUWk4/n7p9+/aZRo0amb59+5qMjAzz/fffmw8//NB8++23Vs24ceNMZGSkee+998zmzZvN9ddfb5o0aWKOHDli1XTr1s1ccMEF5vPPPzeffPKJOeecc8xtt91mxyE51tixY02dOnXMokWLzM6dO828efNMzZo1zZQpU6wa+rriPvjgA/PYY4+Z+fPnG0lmwYIFXusro0/z8vJMTEyMSUlJMVu3bjVvv/22CQsLM6+88sopt5/Q5GDt27c3gwYNst4XFxebuLg4k5qaamOr/FtOTo6RZFatWmWMMSY3N9dUq1bNzJs3z6r56quvjCSTnp5ujDn2H3lAQIDJysqyal5++WUTERFh8vPzT+8BONyBAwfMueeea9LS0sxll11mhSb6uXI8/PDDpmPHjidc7/F4TGxsrHnuueesZbm5uSYkJMS8/fbbxhhjtm/fbiSZdevWWTVLliwxLpfL/PTTT1XXeD+TnJxs7rrrLq9lN910k0lJSTHG0NeV4fjQVFl9+tJLL5latWp5/bvx8MMPm2bNmp1ymxmec6iCggJt2LBBiYmJ1rKAgAAlJiYqPT3dxpb5t7y8PElS7dq1JUkbNmxQYWGhVz83b95cDRs2tPo5PT1drVu3VkxMjFWTlJQkt9utbdu2ncbWO9+gQYOUnJzs1Z8S/VxZFi5cqLZt2+qWW25RdHS0LrroIr322mvW+p07dyorK8urnyMjIxUfH+/Vz1FRUWrbtq1Vk5iYqICAAGVkZJy+g3G4Sy+9VMuWLdM333wjSdq8ebM+/fRTXX311ZLo66pQWX2anp6uzp07Kzg42KpJSkpSZmam9u/ff0pt5IG9DvXrr7+quLjY6wtEkmJiYvT111/b1Cr/5vF4NGzYMHXo0EGtWrWSJGVlZSk4OFhRUVFetTExMcrKyrJqyvs9lKzDMXPmzNEXX3yhdevWlVlHP1eO77//Xi+//LKGDx+uRx99VOvWrdP999+v4OBg9enTx+qn8vqxdD9HR0d7rQ8KClLt2rXp51IeeeQRud1uNW/eXIGBgSouLtbYsWOVkpIiSfR1FaisPs3KylKTJk3KbKNkXa1atf50GwlN+NsYNGiQtm7dqk8//dTupvzl7NmzR0OHDlVaWppCQ0Ptbs5flsfjUdu2bfXMM89Iki666CJt3bpV06dPV58+fWxu3V/LO++8o1mzZmn27Nk677zztGnTJg0bNkxxcXH09d8Yw3MOVbduXQUGBpaZXZSdna3Y2FibWuW/Bg8erEWLFmnFihU688wzreWxsbEqKChQbm6uV33pfo6NjS3391CyDseG33JycnTxxRcrKChIQUFBWrVqlaZOnaqgoCDFxMTQz5WgXr16atmypdeyFi1aaPfu3ZJ+76eT/bsRGxurnJwcr/VFRUXat28f/VzKQw89pEceeUS9evVS69atdccdd+iBBx5QamqqJPq6KlRWn1blvyWEJocKDg5WmzZttGzZMmuZx+PRsmXLlJCQYGPL/IsxRoMHD9aCBQu0fPnyMqds27Rpo2rVqnn1c2Zmpnbv3m31c0JCgrZs2eL1H2paWpoiIiLKfIH9XXXp0kVbtmzRpk2brFfbtm2VkpJi/Uw/n7oOHTqUuWXGN998o0aNGkmSmjRpotjYWK9+drvdysjI8Orn3NxcbdiwwapZvny5PB6P4uPjT8NR+IfDhw8rIMD7KzIwMFAej0cSfV0VKqtPExIStHr1ahUWFlo1aWlpatas2SkNzUnilgNONmfOHBMSEmJmzpxptm/fbgYOHGiioqK8Zhfh5P7xj3+YyMhIs3LlSvPzzz9br8OHD1s19957r2nYsKFZvny5Wb9+vUlISDAJCQnW+pKp8F27djWbNm0yS5cuNWeccQZT4f9A6dlzxtDPlWHt2rUmKCjIjB071uzYscPMmjXLVK9e3bz11ltWzbhx40xUVJT573//a7788ktzww03lDtl+6KLLjIZGRnm008/Neeee+7fehp8efr06WPq169v3XJg/vz5pm7dumbEiBFWDX1dcQcOHDAbN240GzduNJLMxIkTzcaNG80PP/xgjKmcPs3NzTUxMTHmjjvuMFu3bjVz5swx1atX55YDfwcvvPCCadiwoQkODjbt27c3n3/+ud1N8iuSyn3NmDHDqjly5Ii57777TK1atUz16tXNjTfeaH7++Wev7ezatctcffXVJiwszNStW9f8v//3/0xhYeFpPhr/cnxoop8rx/vvv29atWplQkJCTPPmzc2rr77qtd7j8ZgnnnjCxMTEmJCQENOlSxeTmZnpVfO///3P3HbbbaZmzZomIiLC9OvXzxw4cOB0Hobjud1uM3ToUNOwYUMTGhpqzjrrLPPYY495TWOnrytuxYoV5f6b3KdPH2NM5fXp5s2bTceOHU1ISIipX7++GTduXKW032VMqdubAgAAoFxc0wQAAOADQhMAAIAPCE0AAAA+IDQBAAD4gNAEAADgA0ITAACADwhNAAAAPiA0AcAfcLlceu+99+xuBgCbEZoA/KX17dtX3bt3t7sZAP4CCE0AAAA+IDQB+Nu4/PLLdf/992vEiBGqXbu2YmNjNXr0aK+aHTt2qHPnzgoNDVXLli2VlpZWZjt79uxRz549FRUVpdq1a+uGG27Qrl27JElff/21qlevrtmzZ1v177zzjsLCwrR9+/aqPDwAVYzQBOBv5c0331SNGjWUkZGh8ePH66mnnrKCkcfj0U033aTg4GBlZGRo+vTpevjhh70+X1hYqKSkJIWHh+uTTz7RZ599ppo1a6pbt24qKChQ8+bN9fzzz+u+++7T7t279eOPP+ree+/Vs88+q5YtW9pxyAAqCQ/sBfCX1rdvX+Xm5uq9997T5ZdfruLiYn3yySfW+vbt2+vKK6/UuHHj9NFHHyk5OVk//PCD4uLiJElLly7V1VdfrQULFqh79+566623NGbMGH311VdyuVySpIKCAkVFRem9995T165dJUnXXnut3G63goODFRgYqKVLl1r1APxTkN0NAIDT6fzzz/d6X69ePeXk5EiSvvrqKzVo0MAKTJKUkJDgVb9582Z9++23Cg8P91p+9OhRfffdd9b7N954Q02bNlVAQIC2bdtGYAL+AghNAP5WqlWr5vXe5XLJ4/H4/PmDBw+qTZs2mjVrVpl1Z5xxhvXz5s2bdejQIQUEBOjnn39WvXr1/nyjATgCoQkAftOiRQvt2bPHK+R8/vnnXjUXX3yx5s6dq+joaEVERJS7nX379qlv37567LHH9PPPPyslJUVffPGFwsLCqvwYAFQdLgQHgN8kJiaqadOm6tOnjzZv3qxPPvlEjz32mFdNSkqK6tatqxtuuEGffPKJdu7cqZUrV+r+++/Xjz/+KEm699571aBBAz3++OOaOHGiiouL9eCDD9pxSAAqEaEJAH4TEBCgBQsW6MiRI2rfvr3uvvtujR071qumevXqWr16tRo2bKibbrpJLVq0UP/+/XX06FFFRETo3//+tz744AP95z//UVBQkGrUqKG33npLr732mpYsWWLTkQGoDMyeAwAA8AFnmgAAAHxAaAIAAPABoQkAAMAHhCYAAAAfEJoAAAB8QGgCAADwAaEJAADAB4QmAAAAHxCaAAAAfEBoAgAA8AGhCQAAwAeEJgAAAB/8fyfQ/9QgsmVRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "singular_values = np.linalg.svd(train_mat, compute_uv=False, hermitian=False)\n",
    "plt.plot(singular_values)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Singular value spectrum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a1d01ab-b4bd-43b4-8e3a-d60e94ac2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_rank_k_approximation(m: np.ndarray, k: int):\n",
    "    \"\"\"Returns the optimal rank-k reconstruction matrix, using SVD.\"\"\"\n",
    "    \n",
    "    assert 0 < k <= np.min(m.shape), f\"The rank must be in [0, min(m, n)]\"\n",
    "    \n",
    "    U, S, Vh = np.linalg.svd(m, full_matrices=False)\n",
    "    \n",
    "    U_k = U[:, :k]\n",
    "    S_k = S[:k]\n",
    "    Vh_k = Vh[:k]\n",
    "    \n",
    "    return np.dot(U_k * S_k, Vh_k)\n",
    "\n",
    "\n",
    "def matrix_pred_fn(train_recon: np.ndarray, sids: np.ndarray, pids: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        train_recon: (M, N) matrix with predicted values for every (sid, pid) pair.\n",
    "        sids: (D,) vector with integer scientist IDs.\n",
    "        pids: (D,) vector with integer paper IDs.\n",
    "        \n",
    "    Outputs: (D,) vector with predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    return train_recon[sids, pids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfad6c5-2a84-4587-a6da-1232cb8fd9fe",
   "metadata": {},
   "source": [
    "We first obtain the optimal rank-$k$ approximation of the training matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79b10471-3beb-4809-9bb5-191005b8bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recon = opt_rank_k_approximation(train_mat, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df305d5-6b2a-484a-ab63-dd8911870b6c",
   "metadata": {},
   "source": [
    "Then, the values of this matrix reconstruction are the predictions for all (sid, pid)-pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "444142e4-72e5-477c-bdf6-712599c9096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 1.207\n"
     ]
    }
   ],
   "source": [
    "pred_fn = lambda sids, pids: matrix_pred_fn(train_recon, sids, pids)\n",
    "\n",
    "# Evaluate on validation data\n",
    "val_score = evaluate(valid_df, pred_fn)\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c152631-47f5-4b8a-a1a6-90b856d922c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_submission(pred_fn, \"submissions/svd_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc1599-f85a-4684-a608-1c7b98ee1c95",
   "metadata": {},
   "source": [
    "## Learned embeddings\n",
    "\n",
    "Next, we will take a machine learning view of the problem. To each scientist and paper, we assign a $d$-dimensional embedding and we predict the rating that the scientist gives the paper to be their dot product. More formally, let $\\vec{s}_i$ be a scientist embedding and $\\vec{p}_j$ be a paper embedding. Then, we make the following rating prediction for this pair: $$\\tilde{r}_{ij} = \\langle \\vec{s}_i, \\vec{p}_j \\rangle.$$ We view these embeddings as our learnable parameters and train them as we would any other model using the squared error loss function: $$\\ell(\\theta) = \\frac{1}{2} |\\langle \\vec{s}_i, \\vec{p}_j \\rangle - r_{ij}|^2,$$ where $\\theta = \\{ \\vec{s}_i \\}_{i=1}^n \\cup \\{ \\vec{p}_j \\}_{j=1}^m$. The following is an implementation of this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67fcd1d8-93dc-45e7-9706-554b9d1edc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30c1d236-0b95-4a68-9447-4ad7042d5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDotProductModel(nn.Module):\n",
    "    def __init__(self, num_scientists: int, num_papers: int, dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Assign to each scientist and paper an embedding\n",
    "        self.scientist_emb = nn.Embedding(num_scientists, dim)\n",
    "        self.paper_emb = nn.Embedding(num_papers, dim)\n",
    "        \n",
    "    def forward(self, sid: torch.Tensor, pid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            sid: [B,], int\n",
    "            pid: [B,], int\n",
    "        \n",
    "        Outputs: [B,], float\n",
    "        \"\"\"\n",
    "\n",
    "        # Per-pair dot product\n",
    "        return torch.sum(self.scientist_emb(sid) * self.paper_emb(pid), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135658a9-8890-4e1e-a5ae-a500e7fc0da2",
   "metadata": {},
   "source": [
    "Set $d=32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5637bee3-2b0c-425e-b6b1-e13547d5ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model (10k scientists, 1k papers, 32-dimensional embeddings) and optimizer\n",
    "model = EmbeddingDotProductModel(10_000, 1_000, 32).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e866a9d-d5c6-40f1-b27f-d934eb6a5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(df: pd.DataFrame) -> torch.utils.data.Dataset:\n",
    "    \"\"\"Conversion from pandas data frame to torch dataset.\"\"\"\n",
    "    \n",
    "    sids = torch.from_numpy(df[\"sid\"].to_numpy())\n",
    "    pids = torch.from_numpy(df[\"pid\"].to_numpy())\n",
    "    ratings = torch.from_numpy(df[\"rating\"].to_numpy()).float()\n",
    "    return torch.utils.data.TensorDataset(sids, pids, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e05764b4-5103-40fa-8910-3d84ea28a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(train_df)\n",
    "valid_dataset = get_dataset(valid_df)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8240462-5fc0-4c51-8e84-1082da8bf295",
   "metadata": {},
   "source": [
    "Training loop, which we run for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "085034ab-cb5a-444a-bcf4-a505cfd17e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] Train loss=34.174, Valid RMSE=2.649\n",
      "[Epoch 2/5] Train loss=16.796, Valid RMSE=2.242\n",
      "[Epoch 3/5] Train loss=3.175, Valid RMSE=1.156\n",
      "[Epoch 4/5] Train loss=1.029, Valid RMSE=0.986\n",
      "[Epoch 5/5] Train loss=0.878, Valid RMSE=0.954\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train model for an epoch\n",
    "    total_loss = 0.0\n",
    "    total_data = 0\n",
    "    model.train()\n",
    "    for sid, pid, ratings in train_loader:\n",
    "        # Move data to GPU\n",
    "        sid = sid.to(device)\n",
    "        pid = pid.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "\n",
    "        # Make prediction and compute loss\n",
    "        pred = model(sid, pid)\n",
    "        loss = F.mse_loss(pred, ratings)\n",
    "\n",
    "        # Compute gradients w.r.t. loss and take a step in that direction\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Keep track of running loss\n",
    "        total_data += len(sid)\n",
    "        total_loss += len(sid) * loss.item()\n",
    "\n",
    "    # Evaluate model on validation data\n",
    "    total_val_mse = 0.0\n",
    "    total_val_data = 0\n",
    "    model.eval()\n",
    "    for sid, pid, ratings in valid_loader:\n",
    "        # Move data to GPU\n",
    "        sid = sid.to(device)\n",
    "        pid = pid.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "\n",
    "        # Clamp predictions in [1,5], since all ground-truth ratings are\n",
    "        pred = model(sid, pid).clamp(1, 5)\n",
    "        mse = F.mse_loss(pred, ratings)\n",
    "\n",
    "        # Keep track of running metrics\n",
    "        total_val_data += len(sid)\n",
    "        total_val_mse += len(sid) * mse.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{NUM_EPOCHS}] Train loss={total_loss / total_data:.3f}, Valid RMSE={(total_val_mse / total_val_data) ** 0.5:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f051b-6854-4fc4-a380-2f6951768ee4",
   "metadata": {},
   "source": [
    "As we can see, this method already provides an improvement on the validation dataset over the naive SVD method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b73bec67-2ee0-4683-beb1-a102dac072d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.954\n"
     ]
    }
   ],
   "source": [
    "pred_fn = lambda sids, pids: model(torch.from_numpy(sids).to(device), torch.from_numpy(pids).to(device)).clamp(1, 5).cpu().numpy()\n",
    "\n",
    "# Evaluate on validation data\n",
    "with torch.no_grad():\n",
    "    val_score = evaluate(valid_df, pred_fn)\n",
    "\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f42b995-2e08-4cfc-90aa-1b59a7e9fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    make_submission(pred_fn, \"submissions/learned_embedding_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeae800-f6d7-4182-9b7e-aa080e92d87e",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "\n",
    "To further improve the score, students can make use of the information in `train_tbr.csv`, which contains the papers that scientists want to read. Furthermore, students can look into more modern collaborative filtering methods and techniques.\n",
    "\n",
    "Have fun :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb760b8b-e60d-409e-912c-f6dd2f8130bd",
   "metadata": {},
   "source": [
    "## Iterative SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d923e04f-34b9-4f0a-ba08-9187ebc66c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_values_mean(mat: np.ndarray) -> np.ndarray:\n",
    "    return np.nan_to_num(mat, nan=np.nanmean(mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bb1aae6-f340-40f8-a624-ed1ce86d5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = read_data_df()\n",
    "train_mat = read_data_matrix(train_df)\n",
    "observed_mask = ~np.isnan(train_mat)\n",
    "train_mat = impute_values_mean(train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acb15456-5cf1-47b0-9102-1db07842c371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: diff = 154.97935464818312\n",
      "Iteration 5: diff = 79.0508158239096\n",
      "Iteration 10: diff = 51.523547568776124\n",
      "Iteration 15: diff = 36.464002735829794\n",
      "Iteration 20: diff = 27.145072153827787\n",
      "Iteration 25: diff = 21.007244628943045\n",
      "Iteration 30: diff = 16.792048696567594\n",
      "Iteration 35: diff = 13.789365724521977\n",
      "Iteration 40: diff = 11.574714105797074\n",
      "Iteration 45: diff = 9.887230759885245\n"
     ]
    }
   ],
   "source": [
    "max_iter = 50\n",
    "k = 2\n",
    "tol = 1e-4\n",
    "\n",
    "for iteration in range(max_iter):\n",
    "    train_mat_reconstructed = opt_rank_k_approximation(train_mat, k)\n",
    "    \n",
    "    # Update only missing values\n",
    "    new_train_mat = train_mat.copy()\n",
    "    new_train_mat[~observed_mask] = train_mat_reconstructed[~observed_mask]\n",
    "    \n",
    "    # Check convergence: compute change on missing entries\n",
    "    diff = np.linalg.norm(new_train_mat[~observed_mask] - train_mat[~observed_mask])\n",
    "    if iteration % 5 == 0:\n",
    "        print(f\"Iteration {iteration}: diff = {diff}\")  # optionally monitor progress\n",
    "    if diff < tol:\n",
    "        break\n",
    "    train_mat = new_train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23d7e5d0-3bc9-4382-b502-3dc4ddff7969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 1.202\n"
     ]
    }
   ],
   "source": [
    "pred_fn = lambda sids, pids: matrix_pred_fn(train_recon, sids, pids)\n",
    "\n",
    "# Evaluate on validation data\n",
    "val_score = evaluate(valid_df, pred_fn)\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b7e288b-eecc-4bba-b3cb-3042e402bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(pred_fn, \"submissions/iter_svd_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76596a3b",
   "metadata": {},
   "source": [
    "## SVDPP\n",
    "\n",
    "Notes:\n",
    "- This notebook is mainly to explore different algorithms and test the implementation of training and evaluation functions.\n",
    "- To crteate submissions, python scripts were used and run as jobs on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ef14e29-0204-4ce6-82e3-8b2886717443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svdpp(train_df, num_factors=20, lr=0.005, reg=0.02, n_epochs=20):\n",
    "    \"\"\"\n",
    "    Train a fast, NumPy‐only SVD++ on train_df (with columns 'sid','pid','rating').\n",
    "    \"\"\"\n",
    "    # 1) remap IDs to 0…N–1\n",
    "    sids = train_df['sid'].unique()\n",
    "    pids = train_df['pid'].unique()\n",
    "    user2ind = {sid: i for i, sid in enumerate(sids)}\n",
    "    item2ind = {pid: i for i, pid in enumerate(pids)}\n",
    "    n_users, n_items = len(sids), len(pids)\n",
    "\n",
    "    # 2) build index arrays once\n",
    "    user_arr   = train_df['sid'].map(user2ind).to_numpy(dtype=np.int32)\n",
    "    item_arr   = train_df['pid'].map(item2ind).to_numpy(dtype=np.int32)\n",
    "    rating_arr = train_df['rating'].to_numpy(dtype=np.float32)\n",
    "\n",
    "    # 3) global mean as float32\n",
    "    mu = np.float32(rating_arr.mean())\n",
    "\n",
    "    # 4) init parameters (float32)\n",
    "    b_u = np.zeros(n_users, np.float32)\n",
    "    b_i = np.zeros(n_items, np.float32)\n",
    "    p   = np.random.normal(0, 0.1, (n_users, num_factors)).astype(np.float32)\n",
    "    q   = np.random.normal(0, 0.1, (n_items, num_factors)).astype(np.float32)\n",
    "    y   = np.random.normal(0, 0.1, (n_items, num_factors)).astype(np.float32)\n",
    "\n",
    "    # 5) build implicit lists\n",
    "    implicit = {u: [] for u in range(n_users)}\n",
    "    for u, i in zip(user_arr, item_arr):\n",
    "        implicit[u].append(i)\n",
    "    Nu_list  = [np.array(implicit[u], dtype=np.int32) for u in range(n_users)]\n",
    "    Nu_count = np.array([len(a) for a in Nu_list], dtype=np.int32)\n",
    "    sqrt_Nu  = np.where(Nu_count>0, np.sqrt(Nu_count, dtype=np.float32), 1.0)\n",
    "\n",
    "    # 6) precompute y_sum[u] = sum_j y[j] / sqrt_Nu[u]\n",
    "    y_sum = np.zeros((n_users, num_factors), np.float32)\n",
    "    for u in range(n_users):\n",
    "        if Nu_count[u]:\n",
    "            y_sum[u] = y[Nu_list[u]].sum(0) / sqrt_Nu[u]\n",
    "\n",
    "    # 7) SGD loop (vectorized implicit updates)\n",
    "    n_ratings = rating_arr.shape[0]\n",
    "    for epoch in range(n_epochs):\n",
    "        perm = np.random.permutation(n_ratings)\n",
    "        for idx in perm:\n",
    "            u = user_arr[idx]; i = item_arr[idx]; r = rating_arr[idx]\n",
    "            imp = y_sum[u]                      # (f,)\n",
    "            pred = mu + b_u[u] + b_i[i] + q[i].dot(p[u] + imp)\n",
    "            err  = r - pred\n",
    "\n",
    "            # biases & factors\n",
    "            b_u[u] += lr * (err - reg * b_u[u])\n",
    "            b_i[i] += lr * (err - reg * b_i[i])\n",
    "            p_old   = p[u].copy()\n",
    "            p[u]   += lr * (err * q[i]   - reg * p[u])\n",
    "            q[i]   += lr * (err * (p_old + imp) - reg * q[i])\n",
    "\n",
    "            # fast implicit update (matches Eq. 15 exactly)\n",
    "            if Nu_count[u]:\n",
    "                coeff = lr * err / sqrt_Nu[u]\n",
    "                idxs  = Nu_list[u]\n",
    "                yj    = y[idxs]\n",
    "                y[idxs] = yj + coeff * q[i] - lr * reg * yj\n",
    "                # update y_sum with Δy_sum = lr*err*q[i] - lr*reg*y_sum[u]\n",
    "                y_sum[u] = y_sum[u] + lr * err * q[i] - lr * reg * y_sum[u]\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} done.\")\n",
    "\n",
    "    return {\n",
    "        'mu':mu, 'b_u':b_u, 'b_i':b_i,\n",
    "        'p':p,   'q':q,   'y':y,\n",
    "        'user2ind':user2ind, 'item2ind':item2ind,\n",
    "        'implicit':implicit, 'num_factors':num_factors\n",
    "    }\n",
    "\n",
    "\n",
    "def svdpp_pred(model, sids, pids):\n",
    "    \"\"\"\n",
    "    Prediction function for trained SVD++ model, with correct baseline fallback.\n",
    "    \"\"\"\n",
    "    mu = model['mu']\n",
    "    user2ind = model['user2ind']\n",
    "    item2ind = model['item2ind']\n",
    "    b_u = model['b_u']\n",
    "    b_i = model['b_i']\n",
    "    p = model['p']\n",
    "    q = model['q']\n",
    "    y = model['y']\n",
    "    num_factors = model['num_factors']\n",
    "    implicit = model['implicit']\n",
    "    \n",
    "    preds = []\n",
    "    for sid, pid in zip(sids, pids):\n",
    "        # start with any known baseline terms\n",
    "        pred = mu\n",
    "        if sid in user2ind:\n",
    "            u = user2ind[sid]\n",
    "            pred += b_u[u]\n",
    "        if pid in item2ind:\n",
    "            i = item2ind[pid]\n",
    "            pred += b_i[i]\n",
    "        # only add the latent term if both user and item are known\n",
    "        if (sid in user2ind) and (pid in item2ind):\n",
    "            Nu = implicit[u]\n",
    "            sqrt_Nu = np.sqrt(len(Nu)) if Nu else 1.0\n",
    "            imp_sum = np.sum(y[Nu, :], axis=0) / sqrt_Nu if Nu else np.zeros(num_factors)\n",
    "            pred += np.dot(q[i], p[u] + imp_sum)\n",
    "        preds.append(pred)\n",
    "    # convert to ndarray and clip to [1,5]\n",
    "    preds = np.array(preds, dtype=np.float32)\n",
    "    return np.clip(preds, 1.0, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d3dd3b-02f6-44cd-86ba-3c753b3d62a3",
   "metadata": {},
   "source": [
    "### Adjust the pred function to add a fixed score amount if scientist want to read the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22434e5-5d36-470c-89aa-8328224176e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TBR data and build a lookup set\n",
    "tbr_df = pd.read_csv(os.path.join(DATA_DIR, \"train_tbr.csv\"))  # columns: sid, pid\n",
    "tbr_pairs = set(zip(tbr_df['sid'], tbr_df['pid']))\n",
    "\n",
    "# Wrap existing pred fn to apply the boost\n",
    "def svdpp_pred_with_tbr_and_cap(model, sids, pids, boost_pairs, boost=0.5, cap=5.0):\n",
    "    # 1) get base SVD++ predictions\n",
    "    base_preds = svdpp_pred(model, sids, pids)\n",
    "    \n",
    "    # 2) add boost for any (sid, pid) in the “to‐be‐read” set\n",
    "    for idx, (sid, pid) in enumerate(zip(sids, pids)):\n",
    "        if (sid, pid) in boost_pairs:\n",
    "            base_preds[idx] += boost\n",
    "    \n",
    "    # 3) cap at the rating ceiling\n",
    "    np.clip(base_preds, None, cap, out=base_preds)\n",
    "    \n",
    "    return base_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161fe949-ef91-4e37-a3fa-fdc6bdc8b492",
   "metadata": {},
   "source": [
    "### Main routine for training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51d2c041-a19a-4f5d-8a76-884be0966f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training and validation data using provided helper function.\n",
    "train_df, valid_df = read_data_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d31e609-9304-49e6-9845-93870a573bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 done.\n"
     ]
    }
   ],
   "source": [
    "# Train the SVD++ model a single epoch to assess running time\n",
    "model = train_svdpp(train_df, num_factors=20, lr=0.005, reg=0.02, n_epochs=1) # ~30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3de59907-d03b-4341-8d84-c128698be55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 done.\n",
      "Epoch 2/10 done.\n",
      "Epoch 3/10 done.\n",
      "Epoch 4/10 done.\n",
      "Epoch 5/10 done.\n",
      "Epoch 6/10 done.\n",
      "Epoch 7/10 done.\n",
      "Epoch 8/10 done.\n",
      "Epoch 9/10 done.\n",
      "Epoch 10/10 done.\n"
     ]
    }
   ],
   "source": [
    "# Train the SVD++ model (adjust hyperparameters as needed).\n",
    "# Best hyperparameter configurations found include:\n",
    "# fac=50,  lr=0.005, reg=0.05, ep=45\n",
    "# fac=50,  lr=0.01,  reg=0.05, ep=20\n",
    "# fac=100, lr=0.01,  reg=0.05, ep=20\n",
    "# fac=20,  lr=0.005, reg=0.02, ep=20\n",
    "model = train_svdpp(train_df, num_factors=50, lr=0.005, reg=0.05, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d80e817e-8585-4eaa-ba24-86dd619fea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.881\n"
     ]
    }
   ],
   "source": [
    "# Define the prediction function for evaluation/submission.\n",
    "svdpp_fn = lambda sids, pids: svdpp_pred(model, sids, pids)\n",
    "\n",
    "# Evaluate on validation data.\n",
    "val_rmse = evaluate(valid_df, svdpp_fn)\n",
    "print(f\"Validation RMSE: {val_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04995781-062a-4fbd-8ba9-52f57c1a017b",
   "metadata": {},
   "source": [
    "### Prediction with adding of boost for paper read-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f33e7ee-69f9-4535-8518-55aee98446f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE (with TBR boost): 0.881\n"
     ]
    }
   ],
   "source": [
    "# Define a new eval‐able function\n",
    "svdpp_tbr_cap_fn = lambda sids, pids: svdpp_pred_with_tbr_and_cap(\n",
    "    model, sids, pids, tbr_pairs, boost=0.5, cap=5.0\n",
    ")\n",
    "\n",
    "# Run usual evaluation\n",
    "val_rmse_tbr = evaluate(valid_df, svdpp_tbr_cap_fn)\n",
    "print(f\"Validation RMSE (with TBR boost): {val_rmse_tbr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6cf6a1-1517-4fba-95a7-adb46e6bec22",
   "metadata": {},
   "source": [
    "Note: For actual submissions, we should train on the entire data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae0b76a0-40cd-4a21-942d-c36f49dc8775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation hits in TBR: 0 / 282047\n",
      "Boost applied to 0 records\n"
     ]
    }
   ],
   "source": [
    "# How many of the validation pairs are actually in the TBR set?\n",
    "val_pairs = set(zip(valid_df['sid'], valid_df['pid']))\n",
    "n_overlap = len(val_pairs & tbr_pairs)\n",
    "print(f\"Validation hits in TBR: {n_overlap} / {len(val_pairs)}\")\n",
    "\n",
    "# Or in your pred-fn itself, count how many boosts we actually apply:\n",
    "count = 0\n",
    "for sid,pid in zip(valid_df['sid'], valid_df['pid']):\n",
    "    if (sid,pid) in tbr_pairs:\n",
    "        count += 1\n",
    "print(\"Boost applied to\", count, \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357f3842-a5c5-4bab-b044-f9f0ec1d7815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission hits in TBR: 0 / 1128187\n",
      "Boost applied to 0 records\n"
     ]
    }
   ],
   "source": [
    "# Read submission dataset\n",
    "df_sub = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "# Split sid_pid into sid and pid columns\n",
    "df_sub[[\"sid\", \"pid\"]] = df_sub[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "df_sub = df_sub.drop(\"sid_pid\", axis=1)\n",
    "df_sub[\"sid\"] = df_sub[\"sid\"].astype(int)\n",
    "df_sub[\"pid\"] = df_sub[\"pid\"].astype(int)\n",
    "\n",
    "# How many of the submission pairs are actually in the TBR set?\n",
    "sub_pairs = set(zip(df_sub['sid'], df_sub['pid']))\n",
    "n_overlap = len(sub_pairs & tbr_pairs)\n",
    "print(f\"Submission hits in TBR: {n_overlap} / {len(sub_pairs)}\")\n",
    "\n",
    "# Or in your pred-fn itself, count how many boosts we actually apply:\n",
    "count = 0\n",
    "for sid,pid in zip(df_sub['sid'], df_sub['pid']):\n",
    "    if (sid,pid) in tbr_pairs:\n",
    "        count += 1\n",
    "print(\"Boost applied to\", count, \"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1df5b",
   "metadata": {},
   "source": [
    "## Full Hyprid method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b86e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def compute_baseline(user_arr, item_arr, rating_arr, n_users, n_items, reg=20.0, n_epochs=10, lr=0.005):\n",
    "    \"\"\"\n",
    "    Compute global mean mu, user biases bu, and item biases bi.\n",
    "    Solves min_bu,bi sum((r_ui - mu - bu_u - bi_i)^2) + reg*(bu^2 + bi^2).\n",
    "    \"\"\"\n",
    "    mu = np.mean(rating_arr, dtype=np.float32)\n",
    "    bu = np.zeros(n_users, np.float32)\n",
    "    bi = np.zeros(n_items, np.float32)\n",
    "    for _ in range(n_epochs):\n",
    "        for u, i, r in zip(user_arr, item_arr, rating_arr):\n",
    "            pred = mu + bu[u] + bi[i]\n",
    "            err = r - pred\n",
    "            bu[u] += lr * (err - reg * bu[u])\n",
    "            bi[i] += lr * (err - reg * bi[i])\n",
    "    return mu, bu, bi\n",
    "\n",
    "\n",
    "def compute_item_neighbors(user_arr, item_arr, rating_arr, mu, bu, bi, n_items, k=300, shrink=100.0):\n",
    "    \"\"\"\n",
    "    Compute top-k neighbors for each item using shrunk Pearson correlation on residuals.\n",
    "    Returns: dict i -> list of neighbor item indices (length k or fewer).\n",
    "    \"\"\"\n",
    "    # Build item->(user->residual) map\n",
    "    item_users = defaultdict(dict)\n",
    "    for u, i, r in zip(user_arr, item_arr, rating_arr):\n",
    "        resid = r - (mu + bu[u] + bi[i])\n",
    "        item_users[i][u] = resid\n",
    "\n",
    "    neighbors = {}\n",
    "    for i in range(n_items):\n",
    "        sims = []\n",
    "        users_i = item_users[i]\n",
    "        keys_i = set(users_i.keys())\n",
    "        mean_i = 0.0  # zero-mean residuals\n",
    "        var_i = sum(v*v for v in users_i.values())\n",
    "        for j in range(n_items):\n",
    "            if j == i:\n",
    "                continue\n",
    "            users_j = item_users[j]\n",
    "            common = keys_i & set(users_j.keys())\n",
    "            n_common = len(common)\n",
    "            if n_common < 2:\n",
    "                continue\n",
    "            # compute covariance & variances\n",
    "            cov = sum(users_i[u] * users_j[u] for u in common)\n",
    "            var_j = sum(users_j[u]*users_j[u] for u in common)\n",
    "            denom = np.sqrt(var_i * var_j)\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            pearson = cov / denom\n",
    "            shrunk = (n_common / (n_common + shrink)) * pearson\n",
    "            sims.append((shrunk, j))\n",
    "        # pick top-k\n",
    "        sims.sort(reverse=True, key=lambda x: x[0])\n",
    "        neighbors[i] = [j for _, j in sims[:k]]\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def train_hybrid(train_df, valid_df=None,\n",
    "                 num_factors=20,\n",
    "                 lr1=0.007, lr2=0.007, lr3=0.001,\n",
    "                 reg1=0.005, reg2=0.015, reg3=0.015,\n",
    "                 k=300, shrink=100.0,\n",
    "                 n_epochs=30, eval_interval=5,\n",
    "                 seed=42):\n",
    "    \"\"\"\n",
    "    Train the full hybrid model (baseline + SVD++ + neighborhood).\n",
    "    Returns the model dict.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    # remap IDs\n",
    "    sids = train_df['sid'].unique()\n",
    "    pids = train_df['pid'].unique()\n",
    "    user2ind = {sid: u for u, sid in enumerate(sids)}\n",
    "    item2ind = {pid: i for i, pid in enumerate(pids)}\n",
    "    n_users, n_items = len(sids), len(pids)\n",
    "\n",
    "    # index arrays\n",
    "    user_arr   = train_df['sid'].map(user2ind).values.astype(np.int32)\n",
    "    item_arr   = train_df['pid'].map(item2ind).values.astype(np.int32)\n",
    "    rating_arr = train_df['rating'].values.astype(np.float32)\n",
    "\n",
    "    # 1. Baseline\n",
    "    mu, bu, bi = compute_baseline(user_arr, item_arr, rating_arr,\n",
    "                                   n_users, n_items,\n",
    "                                   reg=reg1, n_epochs=10, lr=lr1)\n",
    "\n",
    "    # 2. Implicit feedback\n",
    "    implicit = {u: [] for u in range(n_users)}\n",
    "    for u, i in zip(user_arr, item_arr):\n",
    "        implicit[u].append(i)\n",
    "    Nu_list = [np.array(implicit[u], dtype=np.int32) for u in range(n_users)]\n",
    "    Nu_count = np.array([len(l) for l in Nu_list], dtype=np.int32)\n",
    "    sqrt_Nu = np.where(Nu_count>0, np.sqrt(Nu_count, dtype=np.float32), 1.0)\n",
    "\n",
    "    # 3. Neighborhood structure\n",
    "    neighbors = compute_item_neighbors(user_arr, item_arr, rating_arr,\n",
    "                                       mu=mu, bu=bu, bi=bi,\n",
    "                                       n_items=n_items,\n",
    "                                       k=k, shrink=shrink)\n",
    "\n",
    "    # 4. Initialize parameters\n",
    "    p = np.random.normal(0, 0.1, (n_users, num_factors)).astype(np.float32)\n",
    "    q = np.random.normal(0, 0.1, (n_items, num_factors)).astype(np.float32)\n",
    "    y = np.random.normal(0, 0.1, (n_items, num_factors)).astype(np.float32)\n",
    "    # w and c: dict i -> array(len(neighbors[i]))\n",
    "    w = {i: np.zeros(len(neighbors[i]), np.float32) for i in range(n_items)}\n",
    "    c = {i: np.zeros(len(neighbors[i]), np.float32) for i in range(n_items)}\n",
    "\n",
    "    # 5. Precompute y_sum per user\n",
    "    y_sum = np.zeros((n_users, num_factors), np.float32)\n",
    "    for u in range(n_users):\n",
    "        if Nu_count[u] > 0:\n",
    "            y_sum[u] = y[Nu_list[u]].sum(axis=0) / sqrt_Nu[u]\n",
    "\n",
    "    # 6. Build user->item->rating map for residual lookups\n",
    "    ratings_by_user = [dict() for _ in range(n_users)]\n",
    "    for u, i, r in zip(user_arr, item_arr, rating_arr):\n",
    "        ratings_by_user[u][i] = r\n",
    "\n",
    "    # 7. SGD loop\n",
    "    n_ratings = rating_arr.shape[0]\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        perm = np.random.permutation(n_ratings)\n",
    "        for idx in perm:\n",
    "            u, i, r = user_arr[idx], item_arr[idx], rating_arr[idx]\n",
    "            # SVD++ component\n",
    "            imp = y_sum[u]\n",
    "            svdpp_term = q[i].dot(p[u] + imp)\n",
    "            # Neighborhood terms\n",
    "            Rk = [j for j in neighbors[i] if j in ratings_by_user[u]]\n",
    "            Nk = [j for j in neighbors[i] if j in implicit[u]]\n",
    "            sqrt_Rk = np.sqrt(len(Rk), dtype=np.float32) if Rk else 1.0\n",
    "            sqrt_Nk = np.sqrt(len(Nk), dtype=np.float32) if Nk else 1.0\n",
    "            neigh_explicit = 0.0\n",
    "            for idx_j, j in enumerate(Rk):\n",
    "                neigh_explicit += (ratings_by_user[u][j] - (mu + bu[u] + bi[j])) * w[i][idx_j]\n",
    "            neigh_explicit /= sqrt_Rk\n",
    "            neigh_implicit = 0.0\n",
    "            for idx_j, j in enumerate(Nk):\n",
    "                neigh_implicit += c[i][idx_j]\n",
    "            neigh_implicit /= sqrt_Nk\n",
    "\n",
    "            pred = mu + bu[u] + bi[i] + svdpp_term + neigh_explicit + neigh_implicit\n",
    "            err  = r - pred\n",
    "\n",
    "            # update baseline terms\n",
    "            bu[u] += lr1 * (err - reg1 * bu[u])\n",
    "            bi[i] += lr1 * (err - reg1 * bi[i])\n",
    "            \n",
    "            # update SVD++ factors\n",
    "            p_u_old = p[u].copy()\n",
    "            p[u] += lr2 * (err * q[i]   - reg2 * p[u])\n",
    "            q[i] += lr2 * (err * (p_u_old + imp) - reg2 * q[i])\n",
    "            if Nu_count[u] > 0:\n",
    "                coeff = lr2 * err / sqrt_Nu[u]\n",
    "                for j in Nu_list[u]:\n",
    "                    y[j] += coeff * q[i] - lr2 * reg2 * y[j]\n",
    "                # update y_sum[u]\n",
    "                y_sum[u] = y[Nu_list[u]].sum(axis=0) / sqrt_Nu[u]\n",
    "\n",
    "            # update neighborhood weights\n",
    "            for idx_j, j in enumerate(Rk):\n",
    "                basel_res = ratings_by_user[u][j] - (mu + bu[u] + bi[j])\n",
    "                w[i][idx_j] += lr3 * (err / sqrt_Rk * basel_res - reg3 * w[i][idx_j])\n",
    "            for idx_j, j in enumerate(Nk):\n",
    "                c[i][idx_j] += lr3 * (err / sqrt_Nk - reg3 * c[i][idx_j])\n",
    "\n",
    "        # end of epoch\n",
    "        print(f\"Epoch {epoch}/{n_epochs} completed.\")\n",
    "        if valid_df is not None and (epoch % eval_interval == 0):\n",
    "            model = {\n",
    "                'mu': mu, 'b_u': bu, 'b_i': bi,\n",
    "                'p': p, 'q': q, 'y': y,\n",
    "                'w': w, 'c': c,\n",
    "                'user2ind': user2ind, 'item2ind': item2ind,\n",
    "                'implicit': implicit, 'neighbors': neighbors,\n",
    "                'num_factors': num_factors\n",
    "            }\n",
    "            preds = hybrid_pred(model,\n",
    "                                valid_df['sid'].values,\n",
    "                                valid_df['pid'].values)\n",
    "            rmse = np.sqrt(((valid_df['rating'].values - preds)**2).mean())\n",
    "            print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "\n",
    "    # return final model\n",
    "    return {\n",
    "        'mu': mu, 'b_u': bu, 'b_i': bi,\n",
    "        'p': p, 'q': q, 'y': y,\n",
    "        'w': w, 'c': c,\n",
    "        'user2ind': user2ind, 'item2ind': item2ind,\n",
    "        'implicit': implicit, 'neighbors': neighbors,\n",
    "        'num_factors': num_factors\n",
    "    }\n",
    "\n",
    "\n",
    "def hybrid_pred(model, sids, pids, min_rating=1.0, max_rating=5.0):\n",
    "    \"\"\"\n",
    "    Prediction for hybrid model (Eq. 16).\n",
    "    \"\"\"\n",
    "    mu = model['mu']\n",
    "    b_u, b_i = model['b_u'], model['b_i']\n",
    "    p, q, y = model['p'], model['q'], model['y']\n",
    "    w, c = model['w'], model['c']\n",
    "    user2ind = model['user2ind']\n",
    "    item2ind = model['item2ind']\n",
    "    implicit = model['implicit']\n",
    "    neighbors = model['neighbors']\n",
    "    num_factors = model['num_factors']\n",
    "\n",
    "    preds = []\n",
    "    for sid, pid in zip(sids, pids):\n",
    "        if sid in user2ind and pid in item2ind:\n",
    "            u = user2ind[sid]; i = item2ind[pid]\n",
    "            # SVD++ term\n",
    "            Nu = implicit[u]\n",
    "            sqrt_Nu = np.sqrt(len(Nu)) if Nu else 1.0\n",
    "            imp = np.sum(y[Nu], axis=0) / sqrt_Nu if Nu else np.zeros(num_factors)\n",
    "            svdpp_term = q[i].dot(p[u] + imp)\n",
    "            # neighborhood\n",
    "            Rk = [j for j in neighbors[i] if j in implicit[u]]\n",
    "            Nk = Rk  # since N(u)=R(u)\n",
    "            sqrt_Rk = np.sqrt(len(Rk)) if Rk else 1.0\n",
    "            sqrt_Nk = np.sqrt(len(Nk)) if Nk else 1.0\n",
    "            neigh_explicit = sum((model['implicit_ratings'][u][j] - (mu + b_u[u] + b_i[j])) * w[i][idx]\n",
    "                                 for idx, j in enumerate(Rk)) / sqrt_Rk\n",
    "            neigh_implicit = sum(c[i][idx] for idx, j in enumerate(Nk)) / sqrt_Nk\n",
    "\n",
    "            pred = mu + b_u[u] + b_i[i] + svdpp_term + neigh_explicit + neigh_implicit\n",
    "        else:\n",
    "            pred = mu + 0 + 0\n",
    "        # clip\n",
    "        preds.append(np.clip(pred, min_rating, max_rating))\n",
    "\n",
    "    return np.array(preds, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce824a-c093-4f70-a82b-85a089e88184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the hybrid model a single epoch to assess running time\n",
    "hybrid_model = train_hybrid(train_df, valid_df=None,\n",
    "                 num_factors=50,\n",
    "                 lr1=0.007, lr2=0.007, lr3=0.001,\n",
    "                 reg1=0.005, reg2=0.015, reg3=0.015,\n",
    "                 k=300, shrink=100.0,\n",
    "                 n_epochs=1, eval_interval=5,\n",
    "                 seed=42) # ~30min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113861c4",
   "metadata": {},
   "source": [
    "## Hybrid including to-be-read list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03a9e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_with_tbr(train_df, wish_df, valid_df=None,\n",
    "                         num_factors=20,\n",
    "                         lr1=0.007, lr2=0.007, lr3=0.001,\n",
    "                         reg1=0.005, reg2=0.015, reg3=0.015,\n",
    "                         k=300, shrink=100.0,\n",
    "                         n_epochs=30, eval_interval=5,\n",
    "                         seed=42):\n",
    "    \"\"\"\n",
    "    Train the extended hybrid model with additional \"wish-list\" feedback.\n",
    "    Incorporates baseline + SVD++ + neighborhood + wish-list latent & neighborhood terms.\n",
    "\n",
    "    train_df: DataFrame with ['sid','pid','rating']\n",
    "    wish_df: DataFrame with ['sid','pid'] or set of (sid,pid) pairs\n",
    "    valid_df: optional DataFrame for validation\n",
    "    Returns: model dict\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 1) Build unified user/item ID lists\n",
    "    if isinstance(wish_df, set):\n",
    "        w_sids, w_pids = zip(*wish_df)\n",
    "        w_sids = np.array(w_sids, dtype=int)\n",
    "        w_pids = np.array(w_pids, dtype=int)\n",
    "    else:\n",
    "        w_sids = wish_df['sid'].values.astype(int)\n",
    "        w_pids = wish_df['pid'].values.astype(int)\n",
    "\n",
    "    all_sids = np.unique(np.concatenate([train_df['sid'].values, w_sids]))\n",
    "    all_pids = np.unique(np.concatenate([train_df['pid'].values, w_pids]))\n",
    "    user2ind = {sid: u for u, sid in enumerate(all_sids)}\n",
    "    item2ind = {pid: i for i, pid in enumerate(all_pids)}\n",
    "    n_users, n_items = len(all_sids), len(all_pids)\n",
    "\n",
    "    # 2) Index arrays for training ratings\n",
    "    u_arr = train_df['sid'].map(user2ind).values.astype(np.int32)\n",
    "    i_arr = train_df['pid'].map(item2ind).values.astype(np.int32)\n",
    "    r_arr = train_df['rating'].values.astype(np.float32)\n",
    "\n",
    "    # 3) Baseline biases\n",
    "    mu = r_arr.mean()\n",
    "    bu = np.zeros(n_users, np.float32)\n",
    "    bi = np.zeros(n_items, np.float32)\n",
    "    for _ in range(10):\n",
    "        for u, i, r in zip(u_arr, i_arr, r_arr):\n",
    "            e = r - (mu + bu[u] + bi[i])\n",
    "            bu[u] += lr1 * (e - reg1 * bu[u])\n",
    "            bi[i] += lr1 * (e - reg1 * bi[i])\n",
    "\n",
    "    # 4) Standard implicit feedback (rated items)\n",
    "    implicit = defaultdict(list)\n",
    "    for u, i in zip(u_arr, i_arr):\n",
    "        implicit[u].append(i)\n",
    "    Nu_list = [np.array(implicit[u], dtype=np.int32) for u in range(n_users)]\n",
    "    Nu_count = np.array([len(Nu_list[u]) for u in range(n_users)], dtype=np.int32)\n",
    "    sqrt_Nu = np.where(Nu_count>0, np.sqrt(Nu_count, dtype=np.float32), 1.0)\n",
    "\n",
    "    # 5) Wish-list feedback\n",
    "    wishlist = defaultdict(list)\n",
    "    if isinstance(wish_df, set):\n",
    "        pairs = wish_df\n",
    "    else:\n",
    "        pairs = zip(wish_df['sid'], wish_df['pid'])\n",
    "    for sid, pid in pairs:\n",
    "        if sid in user2ind and pid in item2ind:\n",
    "            wishlist[user2ind[sid]].append(item2ind[pid])\n",
    "    Tw_list = [np.array(wishlist[u], dtype=np.int32) for u in range(n_users)]\n",
    "    Tw_count = np.array([len(Tw_list[u]) for u in range(n_users)], dtype=np.int32)\n",
    "    sqrt_Tw = np.where(Tw_count>0, np.sqrt(Tw_count, dtype=np.float32), 1.0)\n",
    "\n",
    "    # 6) Neighborhood structure\n",
    "    neighbors = compute_item_neighbors(u_arr, i_arr, r_arr,\n",
    "                                       mu, bu, bi,\n",
    "                                       n_items, k=k, shrink=shrink)\n",
    "\n",
    "    # 7) Initialize latent factors & offsets\n",
    "    p = np.random.normal(0,0.1,(n_users,num_factors)).astype(np.float32)\n",
    "    q = np.random.normal(0,0.1,(n_items,num_factors)).astype(np.float32)\n",
    "    y = np.random.normal(0,0.1,(n_items,num_factors)).astype(np.float32)\n",
    "    z = np.random.normal(0,0.1,(n_items,num_factors)).astype(np.float32)\n",
    "    w = {i: np.zeros(len(neighbors[i]),np.float32) for i in range(n_items)}\n",
    "    c = {i: np.zeros(len(neighbors[i]),np.float32) for i in range(n_items)}\n",
    "    d = {i: np.zeros(len(neighbors[i]),np.float32) for i in range(n_items)}\n",
    "\n",
    "    # 8) Precompute sums\n",
    "    y_sum = np.zeros((n_users,num_factors), np.float32)\n",
    "    z_sum = np.zeros((n_users,num_factors), np.float32)\n",
    "    for u in range(n_users):\n",
    "        if Nu_count[u]>0:\n",
    "            y_sum[u] = y[Nu_list[u]].sum(0) / sqrt_Nu[u]\n",
    "        if Tw_count[u]>0:\n",
    "            z_sum[u] = z[Tw_list[u]].sum(0) / sqrt_Tw[u]\n",
    "\n",
    "    # 9) Ratings map for residual lookup\n",
    "    ratings_by_user = [dict() for _ in range(n_users)]\n",
    "    for u, i, r in zip(u_arr, i_arr, r_arr):\n",
    "        ratings_by_user[u][i] = r\n",
    "\n",
    "    # 10) SGD training\n",
    "    n = len(r_arr)\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        perm = np.random.permutation(n)\n",
    "        for idx in perm:\n",
    "            u, i, r = u_arr[idx], i_arr[idx], r_arr[idx]\n",
    "            imp = y_sum[u]\n",
    "            wish = z_sum[u]\n",
    "            svdpp_val = q[i].dot(p[u] + imp + wish)\n",
    "\n",
    "            # neighbors\n",
    "            Rk = [j for j in neighbors[i] if j in ratings_by_user[u]]\n",
    "            Tk = [j for j in neighbors[i] if j in wishlist[u]]\n",
    "            sr = np.sqrt(len(Rk)) if Rk else 1.0\n",
    "            st = np.sqrt(len(Tk)) if Tk else 1.0\n",
    "\n",
    "            nexp = sum((ratings_by_user[u][j] - (mu + bu[u] + bi[j])) * w[i][idx_j]\n",
    "                       for idx_j, j in enumerate(Rk)) / sr\n",
    "            nimp = sum(c[i][idx_j] for idx_j, j in enumerate(Rk)) / sr\n",
    "            nwish = sum(d[i][idx_j] for idx_j, j in enumerate(Tk)) / st\n",
    "\n",
    "            pred = mu + bu[u] + bi[i] + svdpp_val + nexp + nimp + nwish\n",
    "            err  = r - pred\n",
    "\n",
    "            # update biases\n",
    "            bu[u] += lr1 * (err - reg1 * bu[u])\n",
    "            bi[i] += lr1 * (err - reg1 * bi[i])\n",
    "\n",
    "            # update factors\n",
    "            old_pu = p[u].copy()\n",
    "            p[u] += lr2 * (err * q[i]   - reg2 * p[u])\n",
    "            q[i] += lr2 * (err * (old_pu + imp + wish) - reg2 * q[i])\n",
    "\n",
    "            # implicit\n",
    "            if Nu_count[u]>0:\n",
    "                coeff = lr2 * err / sqrt_Nu[u]\n",
    "                for j in Nu_list[u]:\n",
    "                    y[j] += coeff * q[i] - lr2 * reg2 * y[j]\n",
    "                y_sum[u] = y[Nu_list[u]].sum(0) / sqrt_Nu[u]\n",
    "\n",
    "            # wish-list\n",
    "            if Tw_count[u]>0:\n",
    "                coeff = lr2 * err / sqrt_Tw[u]\n",
    "                for j in Tw_list[u]:\n",
    "                    z[j] += coeff * q[i] - lr2 * reg2 * z[j]\n",
    "                z_sum[u] = z[Tw_list[u]].sum(0) / sqrt_Tw[u]\n",
    "\n",
    "            # neighborhood updates\n",
    "            for idx_j, j in enumerate(Rk):\n",
    "                bas_res = ratings_by_user[u][j] - (mu + bu[u] + bi[j])\n",
    "                w[i][idx_j] += lr3 * (err/sr * bas_res - reg3 * w[i][idx_j])\n",
    "                c[i][idx_j] += lr3 * (err/sr          - reg3 * c[i][idx_j])\n",
    "            for idx_j, j in enumerate(Tk):\n",
    "                d[i][idx_j] += lr3 * (err/st - reg3 * d[i][idx_j])\n",
    "\n",
    "        print(f\"Epoch {epoch}/{n_epochs} done.\")\n",
    "        if valid_df is not None and epoch % eval_interval == 0:\n",
    "            model = dict(mu=mu, bu=bu, bi=bi,\n",
    "                         p=p, q=q, y=y, z=z,\n",
    "                         w=w, c=c, d=d,\n",
    "                         user2ind=user2ind, item2ind=item2ind,\n",
    "                         implicit=implicit, wishlist=wishlist,\n",
    "                         neighbors=neighbors, num_factors=num_factors,\n",
    "                         ratings_by_user=ratings_by_user)\n",
    "            preds = hybrid_pred_with_tbr(model,\n",
    "                                        valid_df['sid'].values,\n",
    "                                        valid_df['pid'].values)\n",
    "            rmse = np.sqrt(((valid_df['rating'].values - preds)**2).mean())\n",
    "            print(f\"Valid RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return dict(mu=mu, bu=bu, bi=bi,\n",
    "                p=p, q=q, y=y, z=z,\n",
    "                w=w, c=c, d=d,\n",
    "                user2ind=user2ind, item2ind=item2ind,\n",
    "                implicit=implicit, wishlist=wishlist,\n",
    "                neighbors=neighbors, num_factors=num_factors,\n",
    "                ratings_by_user=ratings_by_user)\n",
    "\n",
    "\n",
    "def hybrid_pred_with_tbr(model, sids, pids,\n",
    "                         min_rating=1.0, max_rating=5.0):\n",
    "    \"\"\"\n",
    "    Prediction for the extended hybrid model with wish-list.\n",
    "    \"\"\"\n",
    "    mu = model['mu']\n",
    "    bu, bi = model['bu'], model['bi']\n",
    "    p, q, y, z = model['p'], model['q'], model['y'], model['z']\n",
    "    w, c, d = model['w'], model['c'], model['d']\n",
    "    user2ind, item2ind = model['user2ind'], model['item2ind']\n",
    "    implicit = model['implicit']\n",
    "    wishlist = model['wishlist']\n",
    "    neighbors = model['neighbors']\n",
    "    ratings_by_user = model['ratings_by_user']\n",
    "    f = model['num_factors']\n",
    "\n",
    "    preds = []\n",
    "    for sid, pid in zip(sids, pids):\n",
    "        if sid in user2ind and pid in item2ind:\n",
    "            u, i = user2ind[sid], item2ind[pid]\n",
    "            # SVD++ terms\n",
    "            Nu, Tw = implicit[u], wishlist[u]\n",
    "            imp = y[Nu].sum(0)/np.sqrt(len(Nu)) if Nu else np.zeros(f)\n",
    "            wish= z[Tw].sum(0)/np.sqrt(len(Tw)) if Tw else np.zeros(f)\n",
    "            svdpp = q[i].dot(p[u] + imp + wish)\n",
    "            # neighbors\n",
    "            Rk = [j for j in neighbors[i] if j in ratings_by_user[u]]\n",
    "            Tk = [j for j in neighbors[i] if j in wishlist[u]]\n",
    "            sr = np.sqrt(len(Rk)) if Rk else 1.0\n",
    "            st = np.sqrt(len(Tk)) if Tk else 1.0\n",
    "\n",
    "            nexp = sum((ratings_by_user[u][j] - (mu + bu[u] + bi[j])) * w[i][idx_j]\n",
    "                       for idx_j, j in enumerate(Rk)) / sr\n",
    "            nimp = sum(c[i][idx_j] for idx_j, j in enumerate(Rk)) / sr\n",
    "            nwish= sum(d[i][idx_j] for idx_j, j in enumerate(Tk)) / st\n",
    "\n",
    "            pred = mu + bu[u] + bi[i] + svdpp + nexp + nimp + nwish\n",
    "        else:\n",
    "            pred = mu\n",
    "        preds.append(np.clip(pred, min_rating, max_rating))\n",
    "\n",
    "    return np.array(preds, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb6c0de9-5fa1-4784-bd60-cc966f4d17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TBR data and build a lookup set\n",
    "tbr_df = pd.read_csv(os.path.join(DATA_DIR, \"train_tbr.csv\"))  # columns: sid, pid\n",
    "tbr_pairs = set(zip(tbr_df['sid'], tbr_df['pid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67247a07-a67d-4ca4-9c39-4ab4804c69e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 done.\n"
     ]
    }
   ],
   "source": [
    "hybrid_tbr_model = train_hybrid_with_tbr(train_df, tbr_pairs, valid_df=None,\n",
    "                               num_factors=20,\n",
    "                               lr1=0.007, lr2=0.007, lr3=0.001,\n",
    "                               reg1=0.005, reg2=0.015, reg3=0.015,\n",
    "                               k=300, shrink=100.0,\n",
    "                               n_epochs=1, eval_interval=5,\n",
    "                               seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
