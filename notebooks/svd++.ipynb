{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d799b9af-d742-4cac-a937-06102e652812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063fd35-caac-4ced-b13e-b49cfb58d9a2",
   "metadata": {},
   "source": [
    "Make sure that results are reproducible by using a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73627bd-1106-4276-a498-32b44f1b5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5b6d6-37ed-40d1-b651-962c611a22c3",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93bc867-b2d9-4cf7-9bb8-ecb13c663eb3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"\"\n",
    "\n",
    "\n",
    "def read_data_df() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reads in data and splits it into training and validation sets with a 75/25 split.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "\n",
    "    # Split sid_pid into sid and pid columns\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "    \n",
    "    # Split into train and validation dataset\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.25)\n",
    "    return train_df, valid_df\n",
    "\n",
    "\n",
    "def read_data_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Returns matrix view of the training data, where columns are scientists (sid) and\n",
    "    rows are papers (pid).\"\"\"\n",
    "\n",
    "    return df.pivot(index=\"sid\", columns=\"pid\", values=\"rating\").values\n",
    "\n",
    "\n",
    "def evaluate(valid_df: pd.DataFrame, pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        valid_df: Validation data, returned from read_data_df for example.\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs their rating predictions.\n",
    "\n",
    "    Outputs: Validation RMSE\n",
    "    \"\"\"\n",
    "    preds = pred_fn(valid_df[\"sid\"].values, valid_df[\"pid\"].values)\n",
    "    return root_mean_squared_error(valid_df[\"rating\"].values, preds)\n",
    "\n",
    "\n",
    "def make_submission(pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray], filename: os.PathLike):\n",
    "    \"\"\"Makes a submission CSV file that can be submitted to kaggle.\n",
    "\n",
    "    Inputs:\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs a score.\n",
    "        filename: File to save the submission to.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "    # Get sids and pids\n",
    "    sid_pid = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    sids = sid_pid[0]\n",
    "    pids = sid_pid[1]\n",
    "    sids = sids.astype(int).values\n",
    "    pids = pids.astype(int).values\n",
    "    \n",
    "    df[\"rating\"] = pred_fn(sids, pids)\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "092c2bb8-87ed-4821-9f9b-df630ea4fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = read_data_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1753b-173c-4b12-a7e4-eefeb77bf119",
   "metadata": {},
   "source": [
    "### Improve Speed of the svdpp training by\n",
    "1) Avoiding iterrows and Python lists\n",
    "2) Vectorizing the \"implicit\"-feedback and removing the inner Python loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c10c9d0-fdf3-40c1-972b-4caf9de01f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svdpp_fast(train_df, num_factors=20, lr=0.005, reg=0.02, n_epochs=20):\n",
    "    \"\"\"\n",
    "    Train a fast, NumPy‐only SVD++ on train_df (with columns 'sid','pid','rating').\n",
    "    \"\"\"\n",
    "    # 1) remap IDs to 0…N–1\n",
    "    sids = train_df['sid'].unique()\n",
    "    pids = train_df['pid'].unique()\n",
    "    user2ind = {sid: i for i, sid in enumerate(sids)}\n",
    "    item2ind = {pid: i for i, pid in enumerate(pids)}\n",
    "    n_users, n_items = len(sids), len(pids)\n",
    "\n",
    "    # 2) build index arrays once\n",
    "    user_arr   = train_df['sid'].map(user2ind).to_numpy(dtype=np.int32)\n",
    "    item_arr   = train_df['pid'].map(item2ind).to_numpy(dtype=np.int32)\n",
    "    rating_arr = train_df['rating'].to_numpy(dtype=np.float32)\n",
    "\n",
    "    # 3) global mean as float32\n",
    "    mu = np.float32(rating_arr.mean())\n",
    "\n",
    "    # 4) init parameters (float32)\n",
    "    b_u = np.zeros(n_users, np.float32)\n",
    "    b_i = np.zeros(n_items, np.float32)\n",
    "    p   = np.random.normal(0, 0.1, (n_users, num_factors)).astype(np.float32)\n",
    "    q   = np.random.normal(0, 0.1, (n_items, num_factors)).astype(np.float32)\n",
    "    y   = np.random.normal(0, 0.1, (n_items, num_factors)).astype(np.float32)\n",
    "\n",
    "    # 5) build implicit lists\n",
    "    implicit = {u: [] for u in range(n_users)}\n",
    "    for u, i in zip(user_arr, item_arr):\n",
    "        implicit[u].append(i)\n",
    "    Nu_list  = [np.array(implicit[u], dtype=np.int32) for u in range(n_users)]\n",
    "    Nu_count = np.array([len(a) for a in Nu_list], dtype=np.int32)\n",
    "    sqrt_Nu  = np.where(Nu_count>0, np.sqrt(Nu_count, dtype=np.float32), 1.0)\n",
    "\n",
    "    # 6) precompute y_sum[u] = sum_j y[j] / sqrt_Nu[u]\n",
    "    y_sum = np.zeros((n_users, num_factors), np.float32)\n",
    "    for u in range(n_users):\n",
    "        if Nu_count[u]:\n",
    "            y_sum[u] = y[Nu_list[u]].sum(0) / sqrt_Nu[u]\n",
    "\n",
    "    # 7) SGD loop (vectorized implicit updates)\n",
    "    n_ratings = rating_arr.shape[0]\n",
    "    for epoch in range(n_epochs):\n",
    "        start = time.time()\n",
    "        perm = np.random.permutation(n_ratings)\n",
    "        for idx in perm:\n",
    "            u = user_arr[idx]; i = item_arr[idx]; r = rating_arr[idx]\n",
    "            imp = y_sum[u]                      # (f,)\n",
    "            pred = mu + b_u[u] + b_i[i] + q[i].dot(p[u] + imp)\n",
    "            err  = r - pred\n",
    "\n",
    "            # biases & factors\n",
    "            b_u[u] += lr * (err - reg * b_u[u])\n",
    "            b_i[i] += lr * (err - reg * b_i[i])\n",
    "            p_old  = p[u].copy()\n",
    "            p[u]  += lr * (err * q[i]   - reg * p[u])\n",
    "            q[i]  += lr * (err * (p_old + imp) - reg * q[i])\n",
    "\n",
    "            # fast implicit update\n",
    "            if Nu_count[u]:\n",
    "                coeff = lr * err / sqrt_Nu[u]\n",
    "                idxs  = Nu_list[u]    # shape (|Nu|,)\n",
    "                yj    = y[idxs]       # (|Nu|,f)\n",
    "                y[idxs] = yj + coeff*q[i] - lr*reg*yj\n",
    "                y_sum[u] = y_sum[u] + coeff*q[i] - lr*reg*y_sum[u]\n",
    "        end = time.time()\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} done. {end-start:.2f}s elapsed.\")\n",
    "\n",
    "    return {\n",
    "        'mu':mu, 'b_u':b_u, 'b_i':b_i,\n",
    "        'p':p,   'q':q,   'y':y,\n",
    "        'user2ind':user2ind, 'item2ind':item2ind,\n",
    "        'implicit':implicit, 'num_factors':num_factors\n",
    "    }\n",
    "\n",
    "# Prediction function based on the trained SVD++ model.\n",
    "def svdpp_pred(model, sids, pids):\n",
    "    mu = model['mu']\n",
    "    user2ind = model['user2ind']\n",
    "    item2ind = model['item2ind']\n",
    "    b_u = model['b_u']\n",
    "    b_i = model['b_i']\n",
    "    p = model['p']\n",
    "    q = model['q']\n",
    "    y = model['y']\n",
    "    num_factors = model['num_factors']\n",
    "    implicit = model['implicit']\n",
    "    \n",
    "    preds = []\n",
    "    for sid, pid in zip(sids, pids):\n",
    "        if (sid in user2ind) and (pid in item2ind):\n",
    "            u = user2ind[sid]\n",
    "            i = item2ind[pid]\n",
    "            Nu = implicit[u]\n",
    "            sqrt_Nu = np.sqrt(len(Nu)) if Nu else 1.0\n",
    "            imp_sum = np.sum(y[Nu, :], axis=0) / sqrt_Nu if Nu else np.zeros(num_factors)\n",
    "            pred = mu + b_u[u] + b_i[i] + np.dot(q[i], p[u] + imp_sum)\n",
    "        else:\n",
    "            pred = mu  # default to global mean if unknown user/item\n",
    "        preds.append(pred)\n",
    "    return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d3dd3b-02f6-44cd-86ba-3c753b3d62a3",
   "metadata": {},
   "source": [
    "### Adjust the pred function to add a fixed score amount if scientist wanta to read the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22434e5-5d36-470c-89aa-8328224176e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TBR data and build a lookup set\n",
    "tbr_df = pd.read_csv(os.path.join(DATA_DIR, \"train_tbr.csv\"))  # columns: sid, pid\n",
    "tbr_pairs = set(zip(tbr_df['sid'], tbr_df['pid']))\n",
    "\n",
    "# Wrap existing pred fn to apply the boost\n",
    "def svdpp_pred_with_tbr_and_cap(model, sids, pids, boost_pairs, boost=0.5, cap=5.0):\n",
    "    # 1) get base SVD++ predictions\n",
    "    base_preds = svdpp_pred(model, sids, pids)\n",
    "    \n",
    "    # 2) add boost for any (sid, pid) in the “to‐be‐read” set\n",
    "    for idx, (sid, pid) in enumerate(zip(sids, pids)):\n",
    "        if (sid, pid) in boost_pairs:\n",
    "            base_preds[idx] += boost\n",
    "    \n",
    "    # 3) cap at the rating ceiling\n",
    "    np.clip(base_preds, None, cap, out=base_preds)\n",
    "    \n",
    "    return base_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161fe949-ef91-4e37-a3fa-fdc6bdc8b492",
   "metadata": {},
   "source": [
    "### Main routine for training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51d2c041-a19a-4f5d-8a76-884be0966f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training and validation data using provided helper function.\n",
    "train_df, valid_df = read_data_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de59907-d03b-4341-8d84-c128698be55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVD++ model (adjust hyperparameters as needed).\n",
    "# num_factors=20, lr=0.005, reg=0.02, n_epochs=5 --> RMSE: 0.892\n",
    "# num_factors=20, lr=0.005, reg=0.02, n_epochs=10 --> RMSE: 0.880\n",
    "# num_factors=20, lr=0.005, reg=0.02, n_epochs=20 -> RMSE: 0.870\n",
    "# num_factors=50, lr=0.01, reg=0.05, n_epochs=5 --> RMSE: 0.885\n",
    "# model = train_svdpp_fast(train_df, num_factors=50, lr=0.01, reg=0.05, n_epochs=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80e817e-8585-4eaa-ba24-86dd619fea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function for evaluation/submission.\n",
    "# svdpp_fn = lambda sids, pids: svdpp_pred(model, sids, pids)\n",
    "\n",
    "# Evaluate on validation data.\n",
    "# val_rmse = evaluate(valid_df, svdpp_fn)\n",
    "# print(f\"Validation RMSE: {val_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04995781-062a-4fbd-8ba9-52f57c1a017b",
   "metadata": {},
   "source": [
    "### Prediction with adding of boost for paper read-list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6cf6a1-1517-4fba-95a7-adb46e6bec22",
   "metadata": {},
   "source": [
    "For actual submissions, we should train on the entire data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffdc783d-486f-40cd-8bd5-d8a92ac89809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_for_training() -> pd.DataFrame:\n",
    "    \"\"\"Reads in the entire dataset for training purposes (no split into validation).\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "    \n",
    "    # Split sid_pid into sid and pid columns\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5167d4-7b68-413c-b841-4f42d1f51b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = read_data_for_training()\n",
    "\n",
    "model = train_svdpp_fast(full_train_df, num_factors=50, lr=0.005, reg=0.05, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "625efb60-b23c-47a0-9f3a-da3ae7ce68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function for evaluation/submission.\n",
    "svdpp_fn = lambda sids, pids: svdpp_pred(model, sids, pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01e46f08-f219-4d2d-a52f-c24d5f7ce061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new eval‐able function\n",
    "svdpp_tbr_cap_fn = lambda sids, pids: svdpp_pred_with_tbr_and_cap(\n",
    "    model, sids, pids, tbr_pairs, boost=0.0, cap=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baf3dc12-961c-4536-9231-e373d79af6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file.\n",
    "make_submission(svdpp_fn, \"svdpp_submission_30e_noboost.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3e33cec-77f6-4553-8646-907d83e29da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file.\n",
    "make_submission(svdpp_tbr_cap_fn, \"svdpp_50_0.005_0.05_30_0.5.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
